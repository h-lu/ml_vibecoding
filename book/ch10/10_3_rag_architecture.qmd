---
title: "10.3 RAG 系统架构：当大模型记忆不够用时"
---

词嵌入技术为我们提供了将文本“几何化”的强大武器。而近年来，基于这种思想并将其发展到极致的大型语言模型 (LLM)，如 GPT 系列，更是展现出了惊人的语言能力。

然而，这些看似无所不知的 LLM，却有两个致命的“阿喀琉斯之踵”：

1.  **知识是“静态”的**：LLM 的知识完全来自于它的训练数据，这些知识在模型训练完成的那一刻就被“冷冻”了。它不知道训练日期之后发生的任何事情，也无法访问实时的信息。
2.  **缺乏“私有”知识**：LLM 对全世界的公开信息了如指掌，但对你公司内部的规章制度、项目报告、财务数据等私有信息一无所知。直接问它“我们公司今年的差旅报销标准是什么？”，它只会胡言乱语。

如果我们想让 LLM 成为一个能真正解决特定业务问题的“专家”，而不是一个“博而不精”的通才，我们就必须为它找到一种能**实时、动态地获取外部知识**的方法。

**检索增强生成 (Retrieval-Augmented Generation, RAG)**，正是当前解决这个问题最主流、最高效的系统架构。

### RAG 的工作流程

RAG 的核心思想非常优雅：**不要指望模型能“记住”所有知识，而是在回答问题前，先让它去“查找”相关的资料，然后基于查找到的资料来组织答案。** 这和我们人类回答专业问题时的思路如出一辙——先去图书馆或者网上查阅资料，再进行总结。

一个典型的 RAG 系统主要包含以下三个核心环节：

![RAG Workflow](https://python.langchain.com/v0.2/assets/images/rag_indexing-c7c46011a3493b82d023758a74f4949a.png)
*图片来源: LangChain Documentation*

#### 1. 索引 (Indexing) - 建立你的知识库

这个阶段是离线的、一次性的准备工作，目标是**将你的私有文档转化为一个可供快速检索的“知识库”**。

-   **加载 (Load)**：首先，加载你的原始文档（如 PDF, Word, 网站等）。
-   **分割 (Split)**：将长文档分割成更小的、有意义的**块 (Chunks)**。这样做是因为我们通常只需要文档中的一小部分来回答某个特定问题，将整个长文档都传给 LLM 会非常低效且昂贵。
-   **嵌入 (Embed)**：使用一个**嵌入模型 (Embedding Model)**（就像我们上一节学到的那样），将每一个文档块都转换成一个能够代表其语义的向量。
-   **存储 (Store)**：将这些文档块的原始文本及其对应的向量，一起存入一个专门用于高效向量搜索的数据库——**向量数据库 (Vector Database)** 中。

#### 2. 检索 (Retrieval) - 找到相关的“书页”

这个阶段是当用户提出问题时，实时发生的。

-   当用户输入一个问题（Query）时，系统首先使用**同一个嵌入模型**，将这个问题也转换成一个向量。
-   然后，系统拿着这个“问题向量”，去向量数据库中进行**相似性搜索**，找到那些与问题向量在几何空间上最“接近”的文档块向量。
-   这些被找回来的文档块，就是系统认为与回答当前问题最相关的“上下文 (Context)”。

#### 3. 增强生成 (Augmented Generation) - 基于资料回答问题

这是最后一步，也是真正利用到 LLM 强大的语言能力的一步。

-   系统会将用户**原始的问题**和上一步**检索到的所有相关文档块**，一起打包成一个内容更丰富的**提示 (Prompt)**。
-   这个 Prompt 的模板通常看起来像这样：


>    请根据以下提供的上下文来回答问题。
>
>    上下文：
>    [这里是检索到的文档块1的内容]
>    [这里是检索到的文档块2的内容]
>    ...
>
>    问题：[这里是用户的原始问题]
>
>    答案：

-   最后，将这个“增强”过的 Prompt 发送给 LLM。由于 LLM 此时已经拿到了所有必要的“参考资料”，它就能够生成一个既准确又忠于原文的答案了。

### 核心权衡：系统架构师的决策点

设计一个高效的 RAG 系统，需要在多个维度上进行权衡，这是机器学习系统架构师的核心价值所在：

-   **块大小 (Chunk Size)**：这是最重要的权衡点之一。块切得太大，可能会包含很多与问题无关的“噪声”，干扰 LLM 的判断，并增加成本；块切得太小，又可能导致一个完整的语义单元被拆散，丢失重要的上下文信息。
-   **检索数量 (Top-K)**：检索回来的文档块数量（K值）也需要权衡。K 太小，可能遗漏掉关键信息；K 太大，同样会引入噪声，分散模型的“注意力”，并显著增加 API 调用成本。
-   **嵌入模型的选择**：不同的嵌入模型在性能和成本上差异巨大。像 OpenAI 的 `text-embedding-3-large` 这样的闭源模型，通常在语义理解的精度上表现更好，但成本也更高。而一些开源模型（如 `sentence-transformers` 系列）则提供了免费、本地部署的可能，代价是性能上可能会有所妥协。选择哪个模型，取决于你的应用场景对精度和预算的要求。

### 可视化流程图

下面的流程图清晰地总结了 RAG 系统的完整数据流：

```{mermaid}
graph TD
    A[用户问题] --> B{嵌入模型};
    B --> C[问题向量];
    
    subgraph 离线索引
        D[私有文档] --> E{文档加载器};
        E --> F{文本分割器};
        F --> G[文档块];
        G --> H{嵌入模型};
        H --> I[文档块向量];
        I --> J[(向量数据库)];
    end
    
    C --> K{相似性搜索};
    J --> K;
    
    K --> L[相关文档块];
    
    subgraph 在线生成
        A --> M{Prompt 模板};
        L --> M;
        M --> N[增强后的Prompt];
        N --> O[LLM];
        O --> P[最终答案];
    end
```

在下一节，我们将简要地了解一下驱动 LLM 如此强大的核心技术——注意力机制，为我们深入学习 Transformer 架构做好铺垫。
