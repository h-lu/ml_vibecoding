### ch09 审阅报告（已完成）

#### 概览
- 章节：ch09（深度学习的基石）
- 小节清单与定位：
  - ch09/9_1_challenge.qmd
  - ch09/9_2_history.qmd
  - ch09/9_3_neuron_activation.qmd
  - ch09/9_4_fully_connected_network.qmd
  - ch09/9_5_training_engine.qmd
  - ch09/9_6_deep_learning_gears.qmd
  - ch09/9_7_vibe_coding_practice.qmd
  - ch09/9_8_exercises.qmd
- 总体评估：
  - FP：3/4（从“线性到非线性”的叙事链清晰；可补优化器/损失/稳定性的边界条件与反例）
  - VC：4/4（资产与实践闭环明确，读者可“玩起来”）
  - BP：3/4（建议对齐 PyTorch 2.x 实践：BCEWithLogitsLoss、AdamW、AMP、梯度裁剪、学习率调度与 torch.compile）
  - 一句话结论：本章从第一性原理串起神经元、网络、训练与三件“神器”，建议补齐现代训练栈与数值稳定性最佳实践。

#### 证据与问题（按小节）
- 9_1 挑战（`ch09/9_1_challenge.qmd`）
  - 关键论断：线性模型有能力边界；需“曲线”函数近似器。
  - 认可点：资产 `linear_inseparable_closer.html` 支撑直觉。
  - 建议：补一句“用核方法或特征映射也可弥补线性性，但本章选择以可学习的非线性网络统一刻画”的边界说明。

- 9_2 历史（`ch09/9_2_history.qmd`）
  - 认可点：三次浪潮叙事完整，强调数据/算力/算法三因素。
  - 风险与边界：涉及个别年份与奖项表述的事实性条目建议标注“需核验（访问日期 2025-08）”并给官方来源链接；避免将未核实信息当作定论。

- 9_3 神经元与激活（`ch09/9_3_neuron_activation.qmd`）
  - 认可点：从加权和到非线性，直觉清晰；资产 `activation_functions_interactive.html`。
  - 建议（BP）：
    - 强化“输出层与损失”的配套关系：二分类用 Sigmoid+BCELoss 在 AMP 下数值不稳，推荐“无 Sigmoid 的 logits + BCEWithLogitsLoss”。多分类用 CrossEntropyLoss（内部含 LogSoftmax），不要对 logits 先做 Softmax。
    - 简述常见激活的适用：ReLU/LeakyReLU/SiLU/GELU 取舍，配套初始化（Kaiming 对 ReLU，Xavier 对 Tanh/Sigmoid）。

- 9_4 全连接网络（`ch09/9_4_fully_connected_network.qmd`）
  - 认可点：UAT 与深度/宽度的层次化表示解释到位。
  - 建议：强调“特征标准化对 MLP 训练稳定性的价值”；输出层与任务对齐（回归无激活，分类 logits）。

- 9_5 训练引擎（`ch09/9_5_training_engine.qmd`）
  - 认可点：损失/梯度/反传机制叙述清晰。
  - 建议（BP）：
    - 优化器：将“默认首选 Adam”更新为“AdamW（解耦权重衰减）为推荐默认”，并说明 weight_decay 的含义与不对偏置/归一化参数施加衰减的常见做法。
    - 学习率调度：加入 OneCycleLR/CosineAnnealingLR 示例与何时使用；
    - 数值稳定：引入 AMP（autocast + GradScaler）、梯度裁剪（clip_grad_norm_）以防梯度爆炸。

- 9_6 神级装备（`ch09/9_6_deep_learning_gears.qmd`）
  - 认可点：BN/残差/Dropout 的问题—方案—效果框架完整。
  - 建议：
    - BN 放置位置（线性后、非线性前/后）的常见实现差异与实践偏好；
    - 残差分支维度不匹配时的 1x1/线性投影；
    - 推理阶段 Dropout 关闭与 BN 的 eval() 模式说明。

- 9_7 实践（`ch09/9_7_vibe_coding_practice.qmd`）
  - 认可点：任务拆解清晰（数据→网络→损失优化→可视化）。
  - P0（BP 纠偏）：指令使用 `BCELoss`+Sigmoid。建议改为：输出层用单神经元 logits（不接 Sigmoid），损失改 `BCEWithLogitsLoss`；若保留 Sigmoid，则需改用 `BCELoss` 并提示 AMP 下不稳，优先前者。
  - P1：优化器改用 `AdamW`，可选加入 `CosineAnnealingLR/OneCycleLR`；加入 `autocast+GradScaler` 与 `clip_grad_norm_` 示例；可选 `torch.compile(model)` 加速。
  - 资产建议：预渲染“决策边界随迭代演化”的 HTML，避免在线绘图依赖。

- 9_8 练习（`ch09/9_8_exercises.qmd`）
  - 认可点：覆盖概念辨析、架构设计、优化器与“神器”的反事实思考。
  - 建议：新增“CrossEntropyLoss 输入是 logits、无需显式 Softmax”的判断题；补一个“BCEWithLogitsLoss vs BCELoss 在 AMP 场景”的选择题。

#### 修订建议与优先级
- P0（阻断/错误）
  - 9.7 将 `BCELoss`+Sigmoid 改为 `BCEWithLogitsLoss`（或保留 Sigmoid 同时保留 `BCELoss` 并显著提示稳定性风险）。

- P1（重要提升）
  - 训练栈：默认使用 `AdamW`（解耦权重衰减），结合 `CosineAnnealingLR` 或 `OneCycleLR`；
  - 数值与效率：加入 AMP（`autocast`/`GradScaler`）、梯度裁剪（`clip_grad_norm_`）、可选 `torch.compile`；
  - 初始化：ReLU/LeakyReLU/SiLU 对应 Kaiming 初始化；Tanh/Sigmoid 对应 Xavier；
  - 文本事实：历史奖项/年份加“需核验”脚注并附官方来源链接。

- P2（打磨/一致性）
  - 统一术语：logits/激活/损失/温度/权重衰减；
  - 资产：新增“决策边界演化”预渲染 HTML；
  - 可达性：图示配色与 alt 文案，和 ch04 规范一致。

#### 资源与可复现性
- 现有资产：`activation_functions_interactive.html`、`linear_inseparable_closer.html`；建议新增“决策边界演化”预渲染 HTML。
- 代码建议：在 `book/code/ch09/` 增补最小脚本示例：
  - `fcn_moons_bcewithlogits_adamw_amp.py`（含 AMP/裁剪/调度）；
  - `decision_boundary_progress_plotly.py`（生成 HTML）。
- 运行规范：固定 `torch.manual_seed`/`numpy.random.seed`；显式 CPU/GPU 选择；保存为独立 HTML 后以 iframe 引用。

#### 术语与引用（访问日期 2025-08）
- PyTorch 激活与损失：`torch.nn` 非线性激活、`CrossEntropyLoss`/`BCEWithLogitsLoss`（稳定性优于 `BCELoss` 于 AMP）
  - https://pytorch.org/docs/stable/nn.html
  - https://pytorch.org/docs/stable/amp.html
- 优化与调度：`AdamW`、`CosineAnnealingLR`、`OneCycleLR`、SWA/EMA 工具
  - https://pytorch.org/docs/stable/optim.html
- 梯度裁剪与初始化：`torch.nn.utils.clip_grad_norm_`、`torch.nn.init`（Kaiming/Xavier）
  - https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html
  - https://pytorch.org/docs/stable/nn.init.html
- 编译加速：`torch.compile`
  - https://pytorch.org/docs/stable

（历史奖项与年份条目：建议附官方来源并标注“需核验（2025-08）”。）

#### 状态与动作
- 审核状态：已完成（8/8）
- 后续动作：
  - P0：9.7 换用 `BCEWithLogitsLoss` 并调整输出层；
  - P1：在 9.5/9.7 增补 AdamW/调度/AMP/裁剪/compile 示例段落；
  - P2：补“决策边界演化”HTML 资产与术语统一。


