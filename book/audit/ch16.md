### ch16 审阅报告（已完成）

#### 概览
- 章节：ch16（教 AI 明辨是非：强化学习与偏好对齐）
- 小节清单与定位：
  - ch16/16_1_business_challenge.qmd
  - ch16/16_2_first_principle.qmd
  - ch16/16_3_alignment_paths.qmd
  - ch16/16_4_safe_rlhf.qmd
  - ch16/16_5_vibe_coding_practice.qmd
  - ch16/16_6_exercises.qmd
- 总体评估：
  - FP：3/4（“从对错到好坏”与三路径叙事自洽，DPO/KL 直觉与边界清晰；可补前置 SFT 假设与评估闭环）
  - VC：4/4（品牌调性挑战抓人，DPO 实践与红队演练可玩性强）
  - BP：3/4（建议补 TRL/PEFT/bitsandbytes 工程细节、评估与合规、线上训练与推理配置）
  - 一句话结论：本章完成“对齐”心智模型搭建与可操作练习，补齐工程与治理细节即可用于生产视角授课。

#### 证据与问题（按小节）
- 16_1 商业挑战（`ch16/16_1_business_challenge.qmd`）
  - 认可点：以“品牌调性”不可监督为切入，合理牵引至偏好对齐；目标函数从“正确性”到“偏好”的转变明确。
  - 建议：在目标定义中加入可验证验收指标（礼貌/帮助性/无害性 win-rate、拒答率、引文率等）。

- 16_2 第一性原理（`ch16/16_2_first_principle.qmd`）
  - 认可点：以“比较/排序”刻画“好坏”的可学信号，建立偏好学习直觉。
  - 建议（FP）：补充“先 SFT 后偏好优化”的必要性与边界；强调偏好数据分布与 SFT 语料一致性的重要性。

- 16_3 对齐路径（`ch16/16_3_alignment_paths.qmd`）
  - 认可点：RM+PPO（经典）、DPO（现代主流）、RLAIF（前沿）三路线准确；DPO 用 KL 约束贴近参考模型的描述到位。
  - 建议（BP）：
    - 明确 DPO 需要显式 `ref_model` 或等价约束；介绍 TRL `DPOTrainer`/`DPOConfig` 关键参数（`beta`、`loss_type`、`max_prompt_length`、`max_length`）。
    - 提及 Online DPO 与 Judge/Reward Model 选项、参考模型在线同步（`sync_ref_model`）与截断、padding-free batching、FlashAttention-2 支持。

- 16_4 安全 RLHF（`ch16/16_4_safe_rlhf.qmd`）
  - 认可点：以 KL 为“缰绳”、多目标偏好（帮助性/无害性）与安全阈值叙述清晰。
  - 建议（BP）：补 OWASP LLM Top 10 威胁场景映射、红队流程与拒答模板；标注多目标/保守损失（WPO/RPO/label smoothing）的工程入口。

- 16_5 Vibe Coding 实践（`ch16/16_5_vibe_coding_practice.qmd`）
  - 认可点：以 TRL DPO 打造“礼貌”模型的教学设计完整（三阶段）。
  - 建议（P0/BP）：
    - 代码需显式创建并传入 `ref_model`；固定随机种子；推理对比段加 `model.eval()` 与 `torch.no_grad()`；打印 base vs aligned 输出。
    - 资源受限建议：PEFT/LoRA（`peft`）+ 4bit 量化（`bitsandbytes` `nf4`）与 `attn_implementation='flash_attention_2'`（如可用）。
    - 数据格式：内存数据集字段统一为 `prompt/chosen/rejected`，设置 `max_prompt_length/max_length` 防 OOM。

- 16_6 练习（`ch16/16_6_exercises.qmd`）
  - 认可点：设计“对齐投毒”红队实验，直观呈现偏好数据质量对模型行为的放大效应。
  - 建议（BP）：加入评估清单（toxicity/bias、安全拒答率、规则触发率）、日志/审计与回滚策略；引导讨论“谁定义偏好”。

#### 修订建议与优先级
- P0（立即修复/防误用）
  - 在 16.5 实践脚本：
    - 显式构造并传入 `ref_model`；设置 `beta`；对比推理段使用 `eval()/no_grad()`；固定随机种子；打印 base 与 aligned 对比。
    - 补充资源受限配置示例：PEFT/LoRA + 4bit（`BitsAndBytesConfig`），并给出 `max_*length` 截断参数。
  - 在 16.4：增加安全拒答模板与阈值示例；映射 OWASP LLM Top 10 到对齐流程检查点。

- P1（工程化与可复现）
  - 记录训练/推理配置：`accelerate`/DeepSpeed ZeRO-2、FlashAttention-2、padding-free batching、Online DPO 与 judge/reward 切换、参考模型在线同步。
  - 增加评估与观测：离线偏好评测（win-rate）与在线 A/B；对齐后回归测试（事实性/幻觉/拒答率）。
  - 数据治理：偏好数据来源、去偏与交叉核验；红队与人工验收闭环。

- P2（术语统一与扩展阅读）
  - 统一术语：RLHF、RLAIF、DPO、Constitutional AI、KL 约束、参考模型/策略模型；给出页内术语表。
  - 增补“对齐投毒/防御”参考与案例；补充 Online DPO、WPO/RPO/LD-DPO 等变体位置。

#### 资源与可复现性
- 代码/数据/环境：
  - 建议在 `book/code/ch16/` 增补最小可运行 DPO 示例（CPU/小显存可跑），含 `requirements.txt` 与 README；提供可选 4bit+LoRA 配置与 `accelerate` 启动脚本。
  - 推理/评估脚本分离，保存与加载对齐后的适配器（`peft`）。
- 资产检查：
  - 未发现 `assets/ch16/` 目录（非必须）；当前小节未直接引用外部资产，构建安全。

#### 术语与引用（访问日期 2025-08）
- TRL DPO 文档与示例：`/huggingface/trl`（DPOTrainer、DPOConfig、OnlineDPO、PEFT 参考模型策略、loss_type 变体、截断/内存优化、FlashAttention-2、vLLM、ZeRO 等）
- DPO 论文：Rafailov et al., Direct Preference Optimization (arXiv:2305.18290)
- InstructGPT（RLHF 经典）：Ouyang et al., Training language models to follow instructions with human feedback (arXiv:2203.02155)
- Constitutional AI：Bai et al., Harmlessness from AI Feedback (arXiv:2212.08073)
- PPO：Schulman et al., Proximal Policy Optimization (arXiv:1707.06347)
- OWASP LLM Top 10：大型语言模型应用安全风险与对策（官方列表）

#### 状态与动作
- 审核状态：已完成
- 变更记录：新增本章审阅，给出 FP/VC/BP 评分与 P0/P1/P2 修订项；资产检查完成。

