### ch10 审阅报告（已完成）

#### 概览
- 章节：ch10（语言智能：从词袋到大型语言模型）
- 小节清单与定位：
  - ch10/10_1_challenge.qmd
  - ch10/10_2_word_embeddings.qmd
  - ch10/10_3_rag_architecture.qmd
  - ch10/10_4_transformer_attention.qmd
  - ch10/10_5_vibe_coding_practice.qmd
  - ch10/10_6_exercises.qmd
- 总体评估：
  - FP：3/4（从“符号→向量→RAG→注意力”的叙事链条自洽；可补嵌入度量/索引近似/检索评估的边界条件）
  - VC：4/4（挑战牵引+实践拆解+流程图清晰，读者可“玩起来”）
  - BP：3/4（建议补齐嵌入/向量库/检索器/提示约束/合规与观测的工程化要点与引用）
  - 一句话结论：本章完成了从语义几何到 RAG 的“第一性原理→工程架构”过渡，需补工程落地与治理评估细节以达成生产级指引。

#### 证据与问题（按小节）
- 10_1 挑战（`ch10/10_1_challenge.qmd`）
  - 认可点：以企业知识问答为牵引，对“符号→向量”的必要性论证清晰。
  - 建议：补充“词/句/文档级表征”的层级说明；说明为何选择分块与向量检索（上下文长度/成本/延迟）。

- 10_2 词嵌入（`ch10/10_2_word_embeddings.qmd`）
  - 认可点：分布式语义直觉与代数类比（king-man+woman≈queen）讲解清楚，配有可视化资产 `word_embedding_space.html`。
  - 建议（BP）：
    - 明确“静态嵌入（Word2Vec/GloVe）”与“上下文嵌入（BERT/Instructor/Sentence Transformers）”的差异与适用场景；
    - 余弦相似度与内积的度量关系：推荐“先归一化后使用内积=余弦”；
    - 嵌入维度/归一化/批量化与缓存策略；明确中英混合/领域语料下选择合适模型。

- 10_3 RAG 架构（`ch10/10_3_rag_architecture.qmd`）
  - 认可点：三阶段（索引/检索/增强生成）与流程图表述清晰，权衡点（chunk、Top-K、嵌入模型）列举到位。
  - 建议（BP）：
    - 向量库：说明 FAISS（本地内存）与 pgvector（Postgres 插件，生产易运维）的取舍；
    - 索引与近似检索：HNSW/IVF/Flat 的精度-延迟-内存权衡；
    - 重排与混合检索：在召回后可增加 Cross-Encoder 重排或 BM25+向量的混合检索；
    - 缓存：对嵌入与检索结果启用持久化缓存，控制成本与延迟；
    - 评估：引入检索与生成两级评估（hit@k、mAP、基于引用的答案正确率），并建议离线集与 A/B 测试结合。

- 10_4 注意力与 Transformer（`ch10/10_4_transformer_attention.qmd`）
  - 认可点：从“一词多义→动态表征→自注意力”的动机链路清楚，符合“直觉优先”的 FP。
  - 建议：补充“缩放点积注意力”的一句话公式与 Q/K/V 含义，并提示位置编码概念（不必下钻细节）。

- 10_5 实践（`ch10/10_5_vibe_coding_practice.qmd`）
  - 认可点：RAG 端到端骨架（LangChain+FAISS+本地嵌入+模板约束+测试）贴合 VC。
  - P0（BP 纠偏/补强）：
    - 明确 Prompt 中加入“仅依据上下文作答；无信息则回答我不知道”的强约束，禁止幻觉；
    - 在示例中开启“返回 source_documents”并展示出处（可追溯性）。
  - P1：
    - 嵌入：推荐 `sentence-transformers`（开源）或 OpenAI/其他闭源嵌入（质量vs成本），并固定归一化策略与度量；
    - 向量库：演示 FAISS 与 pgvector 的可替换实现；
    - 观测：对接 LangSmith/自建日志，记录检索与生成链路（查询、hit@k、token 成本、延迟分位）。

- 10_6 练习（`ch10/10_6_exercises.qmd`）
  - 认可点：强调失败路径分析、引用来源增强可信度。
  - 建议：新增“Top-K 与 chunk 大小网格搜索实验设计题”；补“重排器（Cross-Encoder）引入前后指标对比”的开放题。

#### 修订建议与优先级
- P0（阻断/错误/高风险）
  - 在 10.5 的 Prompt 模板中加入强约束：仅依据检索上下文回答；无上下文则回答“我不知道”；返回源文档与元数据。

- P1（重要提升）
  - 嵌入与度量：默认对嵌入进行 L2 归一化，选用内积作为相似度（等价余弦）；
  - 向量库与索引：给出 FAISS（Flat/HNSW/IVF）与 pgvector 的选择与参数要点；
  - 检索质量：引入混合检索与重排；
  - 评估与观测：建立离线评估集与在线观测（延迟、成本、召回/精度），对接 LangSmith/自建指标；
  - 合规：对外部 API 明确数据去标识与最小化上传，保存与日志留痕策略。

- P2（打磨/一致性）
  - 术语统一：向量数据库/近似检索/重排/提示模板/上下文长度/引用；
  - 资产：补“RAG 质量对比”与“Top-K/Chunk 网格搜索”两类可视化 HTML（可选）。

#### 资源与可复现性
- 现有资产：`word_embedding_space.html`（可视化）、`glove_subset.py/txt`（示例数据）。
- 建议新增代码（`book/code/ch10/`）：
  - `rag_minimal_langchain_faiss.py`（本地嵌入 + FAISS + 返回 source_docs + 约束式 Prompt）；
  - `rag_eval_offline.py`（构造小型问答对、评估 hit@k 与基于引用的回答正确率）；
  - `pgvector_demo.sql/.py`（可选：在 Postgres+pgvector 中复现索引与查询）。
- 运行规范：固定随机种子；分离 `.env`（API Key）；开启嵌入缓存；生成交互结果为独立 HTML/JSON 并以 iframe/代码块引用。

#### 术语与引用（访问日期 2025-08）
- Transformers / 自注意力与模型使用：
  - https://huggingface.co/docs/transformers
- Sentence-Transformers（文本嵌入）：
  - https://www.sbert.net /
  - https://huggingface.co/sentence-transformers
- 向量检索：FAISS / pgvector：
  - https://github.com/facebookresearch/faiss
  - https://github.com/pgvector/pgvector
- LangChain / LangGraph（RAG 框架与代理）：
  - https://python.langchain.com /
  - https://langchain-ai.github.io/langgraph/
- OpenAI API（闭源嵌入与聊天接口）：
  - https://platform.openai.com/docs

#### 状态与动作
- 审核状态：已完成（6/6）
- 后续动作：
  - P0：在 10.5 的模板与示例中加入“仅依据上下文/未知即我不知道/返回来源”的强约束；
  - P1：补嵌入归一化、检索器/索引参数、混合检索与重排、评估与观测章节小节；
  - P2：统一术语与补可视化资产。


