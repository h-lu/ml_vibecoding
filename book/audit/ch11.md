### ch11 审阅报告（已完成）

#### 概览
- 章节：ch11（注意力革命：Transformer 的内部世界）
- 小节清单与定位：
  - ch11/11_1_challenge.qmd
  - ch11/11_2_attention_mechanism.qmd
  - ch11/11_3_transformer_architecture.qmd
  - ch11/11_4_vibe_coding_practice.qmd
  - ch11/11_5_exercises.qmd
- 总体评估：
  - FP：3/4（从 RNN 瓶颈→自注意力→编码器模块的推理链条完整；可补“掩码类型/复杂度/长序列变体/解码器-交叉注意力”的边界条件）
  - VC：4/4（互动演示与可视化实践强，读者可“看见并玩起来”）
  - BP：3/4（建议补齐 PyTorch/HF 近期注意力实现与工程优化、长序列与推理加速、可复现实验设定与合规细节）
  - 一句话结论：本章已构建从直觉到模块的清晰心智模型，建议补工程化与长序列/推理优化与安全合规，形成生产视角的闭环。

#### 证据与问题（按小节）
- 11_1 挑战（`ch11/11_1_challenge.qmd`）
  - 认可点：从“顺序依赖→RNN 串行与长依赖/梯度消失→并行需求”的 FP 链条清晰。
  - 建议：补充 BPTT 与教师强制（teacher forcing）在长序列训练/推理错配的局限，作为注意力革命的现实动因之一。

- 11_2 注意力机制（`ch11/11_2_attention_mechanism.qmd`）
  - 认可点：Q/K/V 三元、缩放点积与 Softmax、数据库类比直观；包含 `√d_k` 缩放与权重归一化阐述。
  - 建议（BP）：
    - 明确复杂度与资源边界：标准自注意力在序列长 n 时时间/显存均为 O(n²)；
    - 掩码与数稳：区分 padding mask 与 causal mask；提示 softmax 数值稳定与 `attention_dropout`；
    - PyTorch/HF 实践：介绍 `scaled_dot_product_attention` 与 `attn_implementation`（`sdpa`/`flash_attention_2`）的可选项与硬件/精度要求；`head_mask` 仅在 eager 有效的注意事项。

- 11_3 Transformer 架构（`ch11/11_3_transformer_architecture.qmd`）
  - 认可点：多头注意力、位置编码（给出正余弦公式）、前馈网络、残差与层归一化阐述完整；补充了 MQA/RoPE/ALiBi 的业界优化视角。
  - 建议（FP/BP）：
    - 架构边界：补充解码器与交叉注意力（encoder-decoder attention）用途、掩码差异；
    - 预归一化 vs 后归一化（Pre-LN/Post-LN）训练稳定性简述；
    - 长序列与推理：简述 KV cache、GQA/MQA 降显存、滑窗/稀疏注意力（Longformer）与可替代范式（Reformer/Transformer-XL）。

- 11_4 Vibe Coding 实践（`ch11/11_4_vibe_coding_practice.qmd`）
  - 认可点：以 BERT 注意力热力图为载体的“可视化内省”非常契合 VC。
  - P0（BP 纠偏/补强）：推理示例务必 `model.eval()` 与 `torch.no_grad()`，并传入 `attention_mask` 以屏蔽 padding 影响；确保 `output_attentions=True`；标注层/头与 token 标签对齐逻辑。
  - P1：可加入 HF 的 `AttentionMaskVisualizer` 或保存多头热力图网格化对比。

- 11_5 练习（`ch11/11_5_exercises.qmd`）
  - 认可点：引导多头直觉与位置编码必要性思辨；设计歧义句可视化任务贴合学习目标。
  - 建议：新增两题——（1）在不同 Top-k 头裁剪下任务性能/可视化差异；（2）对比绝对 PE 与 RoPE/ALiBi 在长序列外推的注意力分布与质量差异（实验或思辨）。

#### 修订建议与优先级
- P0（阻断/错误/高风险）
  - 在 11.4 示例中添加：`model.eval()`、`torch.no_grad()`、`attention_mask`；确保 `output_attentions=True` 并标注层/头与 token 对齐。

- P1（重要提升）
  - 明确 O(n²) 复杂度与显存边界；
  - 掩码类型与数值稳定性（padding/causal、softmax 稳定、attention dropout）；
  - 工程优化：PyTorch SDPA、HF `attn_implementation`（`sdpa`/`flash_attention_2`）、KV cache、MQA/GQA；
  - 长序列：Longformer/Transformer-XL/Reformer 的思路对比；
  - 架构补全：简述解码器与交叉注意力职责；
  - 预归一化（Pre-LN）与训练稳定的推荐实践。

- P2（打磨/一致性）
  - 统一术语：Q/K/V、缩放点积注意力、掩码、位置编码（绝对/相对/RoPE/ALiBi）、多/单/多查询（MHA/MQA/GQA）、KV cache；
  - 资产：可补多头热力图对比图、RoPE/ALiBi 直观示意。

#### 资源与可复现性
- 环境建议：固定随机种子；GPU 上使用 `torch.float16/bfloat16` 并根据硬件启用 `flash-attn`；显式记录 `attn_implementation`、batch/seq 长度与显存占用。
- 资产检查：`book/assets/ch11/` 目前包含 `convolution_animation.html` 等与 CNN 相关资产（疑似放错章节）；非阻断，建议迁移至 `assets/ch12/` 并为本章增补注意力可视化导出图（P2）。
- 可选示例扩展：增加多头热力图网格对比脚本与导出 HTML。

#### 术语与引用（访问日期 2025-08）
- Transformer 原论文：[Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- 相对/旋转位置编码：RoPE（RoFormer）[Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- 线性偏置位置编码：ALiBi [Train Short, Test Long](https://arxiv.org/abs/2108.12409)
- 多查询/分组查询注意力：MQA（Shazeer）[Fast Transformer Decoding](https://arxiv.org/abs/1911.02150)，GQA（Ainslie）[GQA](https://arxiv.org/abs/2305.13245)
- 推理加速：FlashAttention-2 [Tri Dao et al.](https://arxiv.org/abs/2307.08691)
- 长序列变体：[Longformer](https://arxiv.org/abs/2004.05150)，[Reformer](https://arxiv.org/abs/2001.04451)，[Transformer-XL](https://arxiv.org/abs/1901.02860)
- PyTorch SDPA 文档：[scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)
- Hugging Face 注意力实现与可视化：
  - [Attention implementations（sdpa/flash_attention_2）](https://huggingface.co/docs/transformers/main/en/attention_interface)
  - [AttentionMaskVisualizer](https://huggingface.co/docs/transformers/main/en/model_doc/llama#visualize-attention-mask)

#### 状态与动作
- 审核状态：已完成（5/5）
- 后续动作：
  - P0：修正 11.4 推理代码的 eval/无梯度/attention_mask 细节；
  - P1：在 11.2/11.3 补充复杂度/掩码/工程优化/解码器与交叉注意力说明；
  - P2：统一术语并补可视化资产与错位资产迁移建议。


