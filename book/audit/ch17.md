### ch17 审阅报告（已完成）

#### 概览
- 章节：ch17（AI 的“手脚”与“大脑”：设计与构建自主智能体）
- 小节清单与定位：
  - ch17/17_1_business_challenge.qmd
  - ch17/17_2_agent_architectures.qmd
  - ch17/17_3_agent_pillars.qmd
  - ch17/17_4_stateful_langgraph.qmd
  - ch17/17_5_vibe_coding_practice.qmd
  - ch17/17_6_exercises.qmd
- 总体评估：
  - FP：3/4（从“说”到“做”的动机链条清晰，三支柱心智模型完整；可补评估与安全边界）
  - VC：4/4（业务挑战真实、图示充足、LangGraph 实践可玩性强）
  - BP：3/4（建议补生产级 Agent 工程：持久状态/检查点、人类在环、安全与工具沙箱、观测与评估）
  - 一句话结论：本章已完成 Agent 架构的核心心智模型与动手路径，补工程治理与评估后可用于生产视角教学。

#### 证据与问题（按小节）
- 17_1 商业挑战（`ch17/17_1_business_challenge.qmd`）
  - 认可点：从“FAQ 机器人”到“数字员工”的差距刻画准确，需求分解/跨系统工具/状态恢复的痛点到位。
  - 建议：补“验收指标”示例（任务成功率、SLA、人类接管率、重试后成功率、操作安全事件数）。

- 17_2 架构模式（`ch17/17_2_agent_architectures.qmd`）
  - 认可点：单体+工具/路由/顺序流/层次化模式阐释清晰并配以 Mermaid 图。
  - 建议（BP）：在层次化模式加入“任务分解/子目标回收/监督-反思”机制说明；提示可与 LangGraph 的子图/监督者（supervisor）模式映射。

- 17_3 三大支柱（`ch17/17_3_agent_pillars.qmd`）
  - 认可点：规划/工具/记忆的交互闭环图示清晰，将长期记忆与 RAG（ch15）衔接合理。
  - 建议（BP）：工具契约应包含严格参数模式与校验（pydantic/JSON Schema）、失败语义、超时/重试/回退策略；记忆应注明保密/最小化与生命周期管理。

- 17_4 LangGraph 有状态（`ch17/17_4_stateful_langgraph.qmd`）
  - 认可点：状态/节点/边与条件边的解释符合官方理念；对比 `AgentExecutor` 的局限恰当。
  - 建议（P1/BP）：补“持久化与恢复”（CheckpointSaver/Store，如 Redis/SQLite/Postgres）、“人类在环中断（interrupt）”、并行与回退分支实例；指出可使用 LangSmith 观测轨迹。

- 17_5 实践：差旅审批（`ch17/17_5_vibe_coding_practice.qmd`）
  - 认可点：目标明确，展示状态路由与条件边的价值。
  - 建议（P0/BP）：
    - 安装命令建议核对包名：pip 包通常为 `langchain-deepseek`（导入名 `langchain_deepseek`），安装行应为：`pip install langgraph langchain-deepseek langchain`；同时给出替代模型提供方以避免单厂依赖。
    - 增补最小可运行示例的“检查点/恢复”“人类在环（等待输入）”“失败回退与重试”片段；对外部工具增加参数校验与超时。
    - 增补运行所需环境变量列表示例与 `.env` 方案，提示敏感信息隔离。

- 17_6 练习（`ch17/17_6_exercises.qmd`）
  - 认可点：对“工作流状态 vs RL 状态”的对比题引导到位；架构挑战契合实际场景。
  - 建议：在练习中要求给出“评估清单”（任务成功率、平均步数、人工干预比、错误类型分布）与“安全检查”（越权调用/敏感操作拦截）。

#### 修订建议与优先级
- P0（立即修复/防误用）
  - 更正 `17_5` 安装命令中的包名为 `langchain-deepseek`（pip 侧横线，导入侧下划线），或提供通用 OpenAI/DeepSeek 双实现说明。
  - 在 `17_5` 示例为外部工具加入参数校验、超时、重试与回退；在敏感操作前加入白名单/确认提示。

- P1（工程化与可复现）
  - LangGraph 工程补强：
    - 持久状态与检查点：引入 CheckpointSaver/Redis Store，实现崩溃恢复与幂等。
    - 人类在环：使用 interrupt/等待节点，形成审批“打回重填”闭环（与练习 17.6 呼应）。
    - 观测与评估：对接 LangSmith 记录轨迹；引入 AgentEval/OpenEvals 进行任务集评测（成功率/步数/错误）。
  - 安全与合规：
    - 工具沙箱与最小权限（文件系统/网络/凭据范围）；速率限制与审计日志。
    - 工具描述最小充分性与禁止提示（避免越权调用）。

- P2（术语统一与扩展）
  - 统一术语：ReAct、Router、Pipeline、Hierarchical、State/Node/Edge、Checkpoint、Interrupt、HITL。
  - 增补对比阅读：ReAct vs 无状态执行器 vs LangGraph；引入多智能体监督者/子图模式示例链接。

#### 资源与可复现性
- 代码/数据/环境：
  - 建议新增 `book/code/ch17/travel_approval_agent.py` 最小可运行脚本（含 State 类型、条件边、工具、检查点、人类在环占位）；`requirements.txt` 标注 `langgraph`, `langchain`, `langchain-deepseek`/`langchain-openai`, `python-dotenv`；README 给出运行步骤与环境变量示例。
- 资产检查：
  - 未发现 `assets/ch17/` 目录（非必须），当前小节以示意图与代码为主，构建安全。

#### 术语与引用（访问日期 2025-08）
- LangGraph 官方文档：状态图/条件边/检查点/人类在环与并行（Context7：`/langchain-ai/langgraph`）
- LangChain 官方文档：工具/路由/记忆/评估与 LangSmith（Context7：`/langchain-ai/langchain`）
- ReAct：Yao et al., ReAct: Synergizing Reasoning and Acting in Language Models（arXiv, 2022）
- Toolformer：Schick et al., Toolformer: Language Models Can Teach Themselves to Use Tools（arXiv, 2023）
- AgentBench：评测 LLM 作为智能体的基准（arXiv）
- WebArena：拟真 Web 环境智能体评测（arXiv, 2023/2024）
- LangGraph Redis：持久化检查点与向量检索（Context7：`/redis-developer/langgraph-redis`）
- OpenEvals / AgentEval：LangChain 评测组件（Context7：`/langchain-ai/openevals`, `/langchain-ai/agentevals`）

#### 状态与动作
- 审核状态：已完成
- 变更记录：新增本章审阅，给出 FP/VC/BP 评分与 P0/P1/P2 修订项；资产检查完成。

