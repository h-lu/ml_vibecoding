---
title: "12.4 Vibe Coding 实践：模型“看见”了什么？"
---

在前面的章节中，我们已经训练了各种模型来对数据进行分类。我们输入一张图片，模型输出一个标签，例如“猫”或“狗”。但一个关键的问题始终萦绕在我们心头：模型在做出这个决策时，它究竟是“看”到了图片的哪个部分？

它是因为看到了猫的尖耳朵和胡须才判断是“猫”，还是仅仅因为图片背景里有它经常在训练集中见过的毛线球？如果我们能“打开”模型的黑箱，一窥它的“思考”过程，将极大地增强我们对模型的信任，并帮助我们诊断它犯错的原因。

本章的 Vibe Coding 实践，我们将挑战一个更高级、更有趣的任务：**模型可解释性 (Model Interpretability)**。我们将使用一种名为 **Grad-CAM (Gradient-weighted Class Activation Mapping)** 的强大技术，来可视化 CNN 模型在进行图像分类时，其“注意力”究竟集中在图像的哪些区域。

### 我们的目标

- **超越“是什么”**：不仅要知道模型预测的是什么，还要知道它是**依据什么**做出的预测。
- **建立直觉**：通过可视化，直观地理解模型的决策依据，将抽象的卷积层与具体的视觉特征联系起来。
- **诊断错误**：学会利用 Grad-CAM 等工具，来分析和理解为什么模型会对某些图片产生错误的分类。

---

### 第一阶段：AI 快速生成 Grad-CAM 可视化代码

我们的任务是：加载一个在 ImageNet 上预训练好的 CNN 模型（例如 ResNet50），然后对于一张给定的图片，生成一个热力图（Heatmap），显示出模型在将其分类为某个特定类别时，最关注图像的哪些区域。

::: {.callout-important title="Vibe Coding 提示"}
**向你的 AI 助手发出指令：**

“我需要使用 PyTorch 来实现 Grad-CAM，以可视化一个预训练的 ResNet50 模型在进行图像分类时的注意力。请帮我完成以下步骤：

1.  **加载模型和图片**：加载 `torchvision.models` 中的 `resnet50` 预训练模型，并将其设置为评估模式。同时，加载一张你选择的图片（例如，一张包含动物或物体的图片），并对其进行符合 ResNet50 输入要求的预处理（缩放、裁剪、归一化）。
2.  **选择目标层**：Grad-CAM 需要作用于模型最后的那个卷积层。对于 ResNet50，这个层通常是 `layer4`。请帮我获取到这个目标层。
3.  **使用 `captum` 库**：PyTorch 有一个官方的可解释性库 `captum`，它内置了 `GradCAM` 的实现。请使用 `captum.attr.LayerGradCam` 来实现。
4.  **计算归因**：实例化 `LayerGradCam`，并调用其 `attribute` 方法，传入预处理后的图片和目标类别ID，来计算激活图。
5.  **可视化**：将生成的激活图（Attribution Map）与原始图片叠加，生成一张热力图。请使用 `captum.attr.visualization` 中的 `visualize_image_attr` 函数来完成这一步。

请确保代码是完整且可以直接运行的。”
:::

*架构师的思考：这个 Prompt 非常清晰地定义了任务目标和实现路径。我们没有要求 AI 从零手写 Grad-CAM 的复杂算法，而是直接指示它使用 `captum` 这个现成的高级库，这正是 Vibe Coding 范式中“利用工具、聚焦目标”的核心思想。AI 在这种任务下，可以成为一个极速的代码生成器。*

---

### 第二阶段：人类解读、质疑与验证

AI 已经为我们生成了代码和第一张热力图。现在，轮到人类智慧登场了。我们的任务不再是写代码，而是像一个侦探一样，去解读、质疑和验证这些可视化结果。

**请你和你的学习小组，围绕以下问题进行探索和思考：**

1.  **“它看对了吗？”**
    -   观察 AI 生成的热力图。模型高亮的区域，是否真的对应着它所预测的那个物体？例如，如果模型预测是“狗”，高亮的区域是在狗的头部和身体上，还是在背景的草地上？
    -   尝试换几张不同的图片（例如，有多个物体的复杂场景），看看模型的注意力是如何变化的。

2.  **“它看错了什么？” (错误诊断)**
    -   **挑战 AI**：故意找一张模型可能会**分类错误**的图片。例如，一张“看起来像猫的狐狸”图片，或者一张“形状像吉他的小提琴”图片。
    -   运行 Grad-CAM，但这次将**目标类别**设置为模型**错误预测**的那个类别。例如，模型把狐狸错认为了“猫”，我们就看它为了论证“猫”这个结论，究竟是关注了狐狸的哪些部位？
    -   同时，我们再将目标类别设置为**正确的类别**（“狐狸”）。对比这两张热力图，我们能否分析出模型犯错的原因？（例如，它可能过度关注了狐狸尖尖的耳朵，这是一个与猫共有的特征，而忽略了狐狸更长的嘴部）。

3.  **“它能看多细？” (细粒度识别)**
    -   找一张包含同一大类、但不同子类的物体的图片，例如，一张同时有“哈士奇”和“金毛寻回犬”的图片。
    -   分别将目标类别设置为“哈士奇”和“金毛寻回犬”，观察模型生成的两张热力图有何不同。它能否在细微之处（如眼睛的颜色、毛发的纹理、脸型）表现出不同的关注点？这能帮助我们判断模型是否真的学到了细粒度的区分特征。

通过以上实践，你将深刻地体会到，可解释性工具不仅仅是为了“好看”。它是一个强大的诊断武器，能帮助我们洞察模型的内在机理，建立对 AI 的信任，并指导我们如何去改进它。这正是机器学习系统架构师在“模型评估与迭代”环节中，不可或缺的一项核心技能。
