---
title: "12.3 视觉 Transformer (ViT)：当 Transformer 开始“看”世界"
---

在 CNN 统治了计算机视觉领域近十年之后，一个来自自然语言处理领域的“跨界者”——Transformer，提出了一个颠覆性的问题：**我们能否像处理一个句子一样，来处理一张图片？**

这个问题在最初听起来有些匪夷所思。CNN 的成功，建立在它为视觉任务精心设计的**归纳偏置 (Inductive Bias)** 之上，例如局部性 (Locality) 和平移不变性 (Translation Invariance)。这些偏置就像是模型在学习前就已经具备的“先验知识”，非常符合我们对视觉世界的直觉。而 Transformer，作为一个为处理序列数据而生的架构，似乎缺少这些专门为图像定制的“天赋”。

然而，在2020年，Google 的研究者们用一篇名为《An Image is Worth 16x16 Words》的论文，给出了一个响亮的回答：可以！他们提出的 **视觉 Transformer (Vision Transformer, ViT)**，成功地将标准 Transformer 架构直接应用于图像分类，并在大规模数据集上取得了超越顶级 CNN 的性能，彻底改变了计算机视觉领域的格局。

### ViT 的核心思想：把图片当成句子

ViT 的核心思想，就是想办法将一张二维的图片，转化成一个一维的“词元”序列 (Sequence of Tokens)，然后将其直接输入给一个标准的 Transformer 编码器。

这个过程主要分为三步：

**1. 图像的“词元化” (Image Patching)**

这是 ViT 最关键的一步。它不再像 CNN 那样用卷积核逐个像素地扫描，而是简单粗暴地将输入的图片（例如 224x224）切割成一系列不重叠的、固定大小的**小块 (Patches)**。

例如，如果每个 Patch 的大小是 16x16 像素，那么一张 224x224 的图片就会被切割成 `(224/16) * (224/16) = 14 * 14 = 196` 个 Patches。

![ViT Patching](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-10-08_at_4.28.22_PM_5x1rSjN.png)
*图片来源: paperswithcode.com*

接着，每一个 Patch 都会被“压平”成一个一维向量（例如，一个 16x16x3 的 Patch 会被压平成一个 768 维的向量），然后通过一个标准的线性变换，将其映射到模型期望的维度（例如 512 维）。

至此，我们就成功地将一张图片，转换成了一个由 196 个“词元”组成的序列。**每一个“词元”，就代表了原始图片的一个“小块”**。

**2. 位置编码 (Positional Encoding)**

和我们在第十一章学到的一样，标准的 Transformer 架构本身并不包含任何关于序列顺序的信息。为了让模型知道每个 Patch 来自于图片的哪个位置，ViT 同样为每一个 Patch 向量添加了一个可学习的**位置编码 (Positional Encoding)**。

这个位置编码向量的作用，就是告诉模型每个“图像词元”的原始空间位置，使得模型能够理解“左上角”、“中心”、“右下角”等空间概念。

**3. Transformer 编码器**

经过以上两步处理后，我们得到了一系列的、包含了内容和位置信息的向量。这个向量序列，就可以被直接送入一个标准的 Transformer 编码器了。

接下来的故事我们就很熟悉了：

-   **多头自注意力机制 (Multi-Head Self-Attention)** 会在所有这些 Patch 之间计算注意力权重。这意味着，模型可以灵活地、动态地学习到图像中**任意两个 Patch 之间**的相互关系，无论它们相距多远。例如，它能直接关联左上角的一只猫耳朵和右下角的一条猫尾巴，形成对“猫”这个物体的全局理解。
-   **前馈网络 (Feed-Forward Network)** 则负责对每个 Patch 的表示进行进一步的非线性变换。
-   通过堆叠多个这样的编码器模块，ViT 能够学习到越来越丰富、越来越抽象的特征表示。

最后，为了进行分类，ViT 在序列的最前面额外添加了一个特殊的、可学习的 `[CLS]` (Classification) 词元。在经过 Transformer 编码器之后，这个 `[CLS]` 词元最终的输出状态，就被认为是整张图片的全局特征表示。我们只需将这个向量送入一个简单的全连接层，就可以得到最终的分类结果。

### CNN vs. ViT：两种哲学的碰撞

CNN 和 ViT 代表了两种看待视觉问题的、截然不同的哲学。

| 特性 | 卷积神经网络 (CNN) | 视觉 Transformer (ViT) |
| :--- | :--- | :--- |
| **核心思想** | 局部特征提取，层次化组合 | 全局关系建模 |
| **归纳偏置** | **强偏置**：局部性、平移不变性被硬编码在架构中。 | **弱偏置**：除了位置编码，几乎没有内置的视觉先验知识。 |
| **数据需求** | 数据效率高，在**中小规模**数据集上表现优异。 | 数据效率低，需要**大规模**数据集（如 ImageNet-21k, JFT-300M）进行预训练，才能学习到视觉的内在规律。 |
| **优势** | 训练快，收敛稳定，对各种尺寸的数据集都表现良好。 | 在超大规模数据集上预训练后，其性能上限通常**高于** CNN，具有更好的扩展性 (Scalability)。 |
| **类比** | 一位经验丰富的**工匠**，使用一套代代相传的、专门为木工活设计的工具（凿子、刨子），高效地制作家具。 | 一位手持通用、强大但没有预设功能的“万能工具”的**学习者**。如果只有少量木材，他可能无从下手；但如果给他一座森林，他能通过学习，最终造出比工匠更宏伟的建筑。 |

::: {.callout-note title="架构师视角"}
CNN 和 ViT 的选择，是一个典型的**架构权衡 (Architectural Trade-off)**。

-   如果你的项目面临**数据量有限**、需要**快速迭代**的场景，那么 CNN 及其丰富的预训练生态系统，通常是更稳健、更高效的选择。
-   如果你有机会接触到**海量的专有数据**，并追求**极致的性能上限**，那么 ViT 强大的学习能力和扩展性，则可能为你带来更大的惊喜。

在当今，许多最先进的模型（SOTA, State-of-the-art）都试图将两者结合起来，例如通过一个 CNN 来提取底层的局部特征，再将其输入给一个 Transformer 来建模全局关系，以期获得两种哲学的共同优势。
:::
