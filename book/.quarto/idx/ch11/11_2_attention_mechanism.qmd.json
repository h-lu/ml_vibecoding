{"title":"11.2 注意力机制：摆脱循环的革命","markdown":{"yaml":{"title":"11.2 注意力机制：摆脱循环的革命"},"headingText":"核心思想：从“信息传递”到“信息聚合”","containsRefs":false,"markdown":"\n\n面对 RNN 的两大根本瓶颈，研究者们提出了一个颠覆性的问题：**我们真的需要“循环”吗？**\n\n我们能否设计一种全新的机制，让序列中的每一个元素，都能够**直接**、**不受距离限制**地看到所有其他元素，并根据当前任务的需要，**动态地**计算出谁对它最重要？\n\n答案就是**注意力机制 (Attention Mechanism)**。而当一个模型用这种机制来处理序列内部的依赖关系时，我们称之为**自注意力 (Self-Attention)**。\n\n\n自注意力彻底抛弃了 RNN“传送带式”的信息传递模型，转向了一种更高效、更强大的**信息聚合 (Information Aggregation)** 模型。\n\n它的核心思想是：对于序列中的每一个元素，其更新后的表示，应该由**整个序列**的所有元素根据一定的权重**加权求和**得到。而这个**权重**，就代表了其他所有元素对于当前这个元素的“重要性”或“注意力”。\n\n最关键的是，这个权重不是固定的，而是由模型**动态计算**出来的。\n\n### 自注意力 (Self-Attention) 的数据库类比\n\n为了从第一性原理层面理解自注意力的计算过程，我们可以使用一个非常直观的类比：把它想象成一次**数据库的查询操作**。\n\n想象序列中的每一个词，都像是一个进入了数据库的人。为了找到与自己相关的信息，每个人都分饰三个角色：\n\n1.  **查询 (Query, Q)**：这是一个向量，代表“**我想要找什么？**”。它是由当前这个词的原始嵌入向量，乘以一个专门的权重矩阵 `WQ` 得到的。\n2.  **键 (Key, K)**：这也是一个向量，代表“**我有什么信息可以被别人查找？**”。它是由当前词的原始嵌入向量，乘以另一个权重矩阵 `WK` 得到的。你可以把它理解为一个词对外展示的“标签”或“索引”。\n3.  **值 (Value, V)**：这还是一个向量，代表“**我实际包含的信息内容是什么？**”。它是由当前词的原始嵌入向量，乘以第三个权重矩阵 `WV` 得到的。\n\n这三个权重矩阵 `WQ`, `WK`, `WV` 是模型在训练过程中需要学习的参数，它们的作用就是教会模型如何根据原始词义，生成最适合用于注意力计算的 Q, K, V 向量。\n\n现在，对于序列中的任意一个词（我们称之为“当前查询者”），它更新自身表示的过程分为三步：\n\n![Attention Calculation](https://jalammar.github.io/images/t/self-attention-output.png)\n*图片来源: \"The Illustrated Transformer\" by Jay Alammar*\n\n#### 第一步：打分 (Score)\n\n当前查询者，会拿着自己的 **Q (查询)** 向量，去和**序列中所有元素（包括它自己）的 K (键)** 向量进行一次“匹配度”计算。这个计算通常就是向量的**点积 (Dot Product)**。\n\n这个分数 `Score = Q · K` 代表了“键”所对应的那个词，与“查询者”的查找意图有多么相关。分数越高，相关性越强。\n\n#### 第二步：归一化 (Softmax)\n\n得到所有元素的分数后，为了方便后续的加权操作，我们需要将这些分数进行归一化。通常会先将分数除以一个缩放因子（通常是 `sqrt(d_k)`，即 K 向量维度的平方根，用以稳定梯度），然后通过一个 **Softmax** 函数。\n\nSoftmax 函数能将一组任意的分数，转化为一组总和为 1 的、非负的**注意力权重 (Attention Weights)**。这就像是当前查询者，明智地将其 100% 的“注意力”预算，分配给了序列中的所有成员。相关性越高的词，分到的注意力权重就越大。\n\n#### 第三步：加权求和 (Weighted Sum)\n\n最后一步，就是用刚刚得到的这组注意力权重，去对**序列中所有元素的 V (值)** 向量进行加权求和。\n\n`最终输出 = Σ (注意力权重_i * V_i)`\n\n这个最终的输出向量，就是“当前查询者”经过一次自注意力计算后，得到的新表示。\n\n这个新表示非常奇妙：它不再仅仅是当前词自己的信息，而是**整个序列根据它自己的“视角”（即它的 Q 向量）进行的一次信息的动态重组和聚合**。它“吸收”了所有它认为重要的上下文信息，同时忽略了那些不相关的信息。\n\n由于上述所有计算（点积、矩阵乘法）都是高度可并行的，自注意力机制彻底摆脱了 RNN 的计算瓶颈。同时，由于每个词都直接与其他所有词进行了交互，长距离依赖问题也迎刃而解。\n\n### 互动演示：观察注意力的流动\n\n下面的简单动画，模拟了自注意力机制的计算过程。请将鼠标悬停在句子中的某个词（查询者）上，观察其他词（键）的颜色变化，颜色越深代表该词获得的注意力权重越高。\n\n<div id=\"attention-vis\" style=\"font-family: sans-serif; padding: 10px; border: 1px solid #ccc; border-radius: 5px;\">\n    <p><strong>句子:</strong> The cat sat on the mat</p>\n    <p><strong>查询 (Query):</strong> <span id=\"query-word\" style=\"background-color: #add8e6; padding: 2px;\">(悬停在下方词语上)</span></p>\n    <p><strong>注意力权重 (Weights):</strong>\n        <span class=\"word\" data-word=\"The\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">The</span>\n        <span class=\"word\" data-word=\"cat\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">cat</span>\n        <span class=\"word\" data-word=\"sat\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">sat</span>\n        <span class=\"word\" data-word=\"on\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">on</span>\n        <span class=\"word\" data-word=\"the\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">the</span>\n        <span class=\"word\" data-word=\"mat\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">mat</span>\n    </p>\n</div>\n\n<script>\ndocument.addEventListener('DOMContentLoaded', () => {\n    const words = document.querySelectorAll('#attention-vis .word');\n    const queryWordSpan = document.getElementById('query-word');\n\n    // Pre-defined attention scores for demonstration\n    const attentionScores = {\n        'The': {'The': 0.8, 'cat': 0.1, 'sat': 0.05, 'on': 0.02, 'the': 0.02, 'mat': 0.01},\n        'cat': {'The': 0.2, 'cat': 0.7, 'sat': 0.1, 'on': 0.0, 'the': 0.0, 'mat': 0.0},\n        'sat': {'The': 0.05, 'cat': 0.4, 'sat': 0.4, 'on': 0.1, 'the': 0.0, 'mat': 0.05},\n        'on':  {'The': 0.0, 'cat': 0.1, 'sat': 0.3, 'on': 0.3, 'the': 0.1, 'mat': 0.2},\n        'the': {'The': 0.1, 'cat': 0.0, 'sat': 0.0, 'on': 0.1, 'the': 0.7, 'mat': 0.1},\n        'mat': {'The': 0.0, 'cat': 0.1, 'sat': 0.3, 'on': 0.3, 'the': 0.1, 'mat': 0.2}\n    };\n\n    words.forEach(word => {\n        word.addEventListener('mouseover', () => {\n            const currentWord = word.dataset.word;\n            queryWordSpan.textContent = currentWord;\n            const scores = attentionScores[currentWord];\n\n            words.forEach(targetWordSpan => {\n                const targetWord = targetWordSpan.dataset.word;\n                const score = scores[targetWord];\n                // Map score (0-1) to a grayscale color (255-0) and then to a blue scale\n                const colorValue = 255 * (1 - score);\n                const blueIntensity = 255 - colorValue;\n                targetWordSpan.style.backgroundColor = `rgba(70, 130, 180, ${score})`; // SteelBlue with opacity\n                targetWordSpan.style.color = score > 0.5 ? 'white' : 'black';\n            });\n        });\n\n        word.addEventListener('mouseout', () => {\n            queryWordSpan.textContent = '(悬停在下方词语上)';\n            words.forEach(w => {\n                w.style.backgroundColor = 'transparent';\n                w.style.color = 'black';\n            });\n        });\n    });\n});\n</script>\n\n请尝试将鼠标悬停在动词 \"sat\" 上，观察它是如何同时给予主语 \"cat\" 和地点 \"mat\" 较高注意力的。再试试悬停在代词 \"The\" 上，观察它主要关注自身和后面的名词 \"cat\"。\n\n通过这个精巧的“查询-键-值”机制，自注意力赋予了模型一种前所未有的、全局的、动态的视角来理解序列。这是构建更强大的 Transformer 模型的基石。\n","srcMarkdownNoYaml":"\n\n面对 RNN 的两大根本瓶颈，研究者们提出了一个颠覆性的问题：**我们真的需要“循环”吗？**\n\n我们能否设计一种全新的机制，让序列中的每一个元素，都能够**直接**、**不受距离限制**地看到所有其他元素，并根据当前任务的需要，**动态地**计算出谁对它最重要？\n\n答案就是**注意力机制 (Attention Mechanism)**。而当一个模型用这种机制来处理序列内部的依赖关系时，我们称之为**自注意力 (Self-Attention)**。\n\n### 核心思想：从“信息传递”到“信息聚合”\n\n自注意力彻底抛弃了 RNN“传送带式”的信息传递模型，转向了一种更高效、更强大的**信息聚合 (Information Aggregation)** 模型。\n\n它的核心思想是：对于序列中的每一个元素，其更新后的表示，应该由**整个序列**的所有元素根据一定的权重**加权求和**得到。而这个**权重**，就代表了其他所有元素对于当前这个元素的“重要性”或“注意力”。\n\n最关键的是，这个权重不是固定的，而是由模型**动态计算**出来的。\n\n### 自注意力 (Self-Attention) 的数据库类比\n\n为了从第一性原理层面理解自注意力的计算过程，我们可以使用一个非常直观的类比：把它想象成一次**数据库的查询操作**。\n\n想象序列中的每一个词，都像是一个进入了数据库的人。为了找到与自己相关的信息，每个人都分饰三个角色：\n\n1.  **查询 (Query, Q)**：这是一个向量，代表“**我想要找什么？**”。它是由当前这个词的原始嵌入向量，乘以一个专门的权重矩阵 `WQ` 得到的。\n2.  **键 (Key, K)**：这也是一个向量，代表“**我有什么信息可以被别人查找？**”。它是由当前词的原始嵌入向量，乘以另一个权重矩阵 `WK` 得到的。你可以把它理解为一个词对外展示的“标签”或“索引”。\n3.  **值 (Value, V)**：这还是一个向量，代表“**我实际包含的信息内容是什么？**”。它是由当前词的原始嵌入向量，乘以第三个权重矩阵 `WV` 得到的。\n\n这三个权重矩阵 `WQ`, `WK`, `WV` 是模型在训练过程中需要学习的参数，它们的作用就是教会模型如何根据原始词义，生成最适合用于注意力计算的 Q, K, V 向量。\n\n现在，对于序列中的任意一个词（我们称之为“当前查询者”），它更新自身表示的过程分为三步：\n\n![Attention Calculation](https://jalammar.github.io/images/t/self-attention-output.png)\n*图片来源: \"The Illustrated Transformer\" by Jay Alammar*\n\n#### 第一步：打分 (Score)\n\n当前查询者，会拿着自己的 **Q (查询)** 向量，去和**序列中所有元素（包括它自己）的 K (键)** 向量进行一次“匹配度”计算。这个计算通常就是向量的**点积 (Dot Product)**。\n\n这个分数 `Score = Q · K` 代表了“键”所对应的那个词，与“查询者”的查找意图有多么相关。分数越高，相关性越强。\n\n#### 第二步：归一化 (Softmax)\n\n得到所有元素的分数后，为了方便后续的加权操作，我们需要将这些分数进行归一化。通常会先将分数除以一个缩放因子（通常是 `sqrt(d_k)`，即 K 向量维度的平方根，用以稳定梯度），然后通过一个 **Softmax** 函数。\n\nSoftmax 函数能将一组任意的分数，转化为一组总和为 1 的、非负的**注意力权重 (Attention Weights)**。这就像是当前查询者，明智地将其 100% 的“注意力”预算，分配给了序列中的所有成员。相关性越高的词，分到的注意力权重就越大。\n\n#### 第三步：加权求和 (Weighted Sum)\n\n最后一步，就是用刚刚得到的这组注意力权重，去对**序列中所有元素的 V (值)** 向量进行加权求和。\n\n`最终输出 = Σ (注意力权重_i * V_i)`\n\n这个最终的输出向量，就是“当前查询者”经过一次自注意力计算后，得到的新表示。\n\n这个新表示非常奇妙：它不再仅仅是当前词自己的信息，而是**整个序列根据它自己的“视角”（即它的 Q 向量）进行的一次信息的动态重组和聚合**。它“吸收”了所有它认为重要的上下文信息，同时忽略了那些不相关的信息。\n\n由于上述所有计算（点积、矩阵乘法）都是高度可并行的，自注意力机制彻底摆脱了 RNN 的计算瓶颈。同时，由于每个词都直接与其他所有词进行了交互，长距离依赖问题也迎刃而解。\n\n### 互动演示：观察注意力的流动\n\n下面的简单动画，模拟了自注意力机制的计算过程。请将鼠标悬停在句子中的某个词（查询者）上，观察其他词（键）的颜色变化，颜色越深代表该词获得的注意力权重越高。\n\n<div id=\"attention-vis\" style=\"font-family: sans-serif; padding: 10px; border: 1px solid #ccc; border-radius: 5px;\">\n    <p><strong>句子:</strong> The cat sat on the mat</p>\n    <p><strong>查询 (Query):</strong> <span id=\"query-word\" style=\"background-color: #add8e6; padding: 2px;\">(悬停在下方词语上)</span></p>\n    <p><strong>注意力权重 (Weights):</strong>\n        <span class=\"word\" data-word=\"The\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">The</span>\n        <span class=\"word\" data-word=\"cat\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">cat</span>\n        <span class=\"word\" data-word=\"sat\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">sat</span>\n        <span class=\"word\" data-word=\"on\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">on</span>\n        <span class=\"word\" data-word=\"the\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">the</span>\n        <span class=\"word\" data-word=\"mat\" style=\"padding: 5px; margin: 2px; border-radius: 3px;\">mat</span>\n    </p>\n</div>\n\n<script>\ndocument.addEventListener('DOMContentLoaded', () => {\n    const words = document.querySelectorAll('#attention-vis .word');\n    const queryWordSpan = document.getElementById('query-word');\n\n    // Pre-defined attention scores for demonstration\n    const attentionScores = {\n        'The': {'The': 0.8, 'cat': 0.1, 'sat': 0.05, 'on': 0.02, 'the': 0.02, 'mat': 0.01},\n        'cat': {'The': 0.2, 'cat': 0.7, 'sat': 0.1, 'on': 0.0, 'the': 0.0, 'mat': 0.0},\n        'sat': {'The': 0.05, 'cat': 0.4, 'sat': 0.4, 'on': 0.1, 'the': 0.0, 'mat': 0.05},\n        'on':  {'The': 0.0, 'cat': 0.1, 'sat': 0.3, 'on': 0.3, 'the': 0.1, 'mat': 0.2},\n        'the': {'The': 0.1, 'cat': 0.0, 'sat': 0.0, 'on': 0.1, 'the': 0.7, 'mat': 0.1},\n        'mat': {'The': 0.0, 'cat': 0.1, 'sat': 0.3, 'on': 0.3, 'the': 0.1, 'mat': 0.2}\n    };\n\n    words.forEach(word => {\n        word.addEventListener('mouseover', () => {\n            const currentWord = word.dataset.word;\n            queryWordSpan.textContent = currentWord;\n            const scores = attentionScores[currentWord];\n\n            words.forEach(targetWordSpan => {\n                const targetWord = targetWordSpan.dataset.word;\n                const score = scores[targetWord];\n                // Map score (0-1) to a grayscale color (255-0) and then to a blue scale\n                const colorValue = 255 * (1 - score);\n                const blueIntensity = 255 - colorValue;\n                targetWordSpan.style.backgroundColor = `rgba(70, 130, 180, ${score})`; // SteelBlue with opacity\n                targetWordSpan.style.color = score > 0.5 ? 'white' : 'black';\n            });\n        });\n\n        word.addEventListener('mouseout', () => {\n            queryWordSpan.textContent = '(悬停在下方词语上)';\n            words.forEach(w => {\n                w.style.backgroundColor = 'transparent';\n                w.style.color = 'black';\n            });\n        });\n    });\n});\n</script>\n\n请尝试将鼠标悬停在动词 \"sat\" 上，观察它是如何同时给予主语 \"cat\" 和地点 \"mat\" 较高注意力的。再试试悬停在代词 \"The\" 上，观察它主要关注自身和后面的名词 \"cat\"。\n\n通过这个精巧的“查询-键-值”机制，自注意力赋予了模型一种前所未有的、全局的、动态的视角来理解序列。这是构建更强大的 Transformer 模型的基石。\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":false,"number-sections":false,"highlight-style":"github","include-in-header":{"text":"<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap\" rel=\"stylesheet\">\n<style>\n/* ChatGPT 风格变量 */\n:root {\n  --chatgpt-primary: #000000;\n  --chatgpt-secondary: #6b7280;\n  --chatgpt-background: #ffffff;\n  --chatgpt-surface: #f7f7f8;\n  --chatgpt-border: #e5e5e5;\n  --chatgpt-accent: #10a37f;\n}\n</style>\n"},"output-file":"11_2_attention_mechanism.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.14","theme":["cosmo","../assets/chatgpt-style.scss"],"fig-cap-location":"bottom","mainfont":"Inter, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Ubuntu, Cantarell, 'Noto Sans', sans-serif, 'Helvetica Neue', Arial, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'","monofont":"Monaco, 'SF Mono', 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace","fontsize":"16px","linestretch":1.6,"code-copy":true,"max-width":"1200px","title":"11.2 注意力机制：摆脱循环的革命"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}