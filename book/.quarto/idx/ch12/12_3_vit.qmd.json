{"title":"12.3 视觉 Transformer (ViT)：当 Transformer 开始“看”世界","markdown":{"yaml":{"title":"12.3 视觉 Transformer (ViT)：当 Transformer 开始“看”世界"},"headingText":"ViT 的核心思想：把图片当成句子","containsRefs":false,"markdown":"\n\n在 CNN 统治了计算机视觉领域近十年之后，一个来自自然语言处理领域的“跨界者”——Transformer，提出了一个颠覆性的问题：**我们能否像处理一个句子一样，来处理一张图片？**\n\n这个问题在最初听起来有些匪夷所思。CNN 的成功，建立在它为视觉任务精心设计的**归纳偏置 (Inductive Bias)** 之上，例如局部性 (Locality) 和平移不变性 (Translation Invariance)。这些偏置就像是模型在学习前就已经具备的“先验知识”，非常符合我们对视觉世界的直觉。而 Transformer，作为一个为处理序列数据而生的架构，似乎缺少这些专门为图像定制的“天赋”。\n\n然而，在2020年，Google 的研究者们用一篇名为《An Image is Worth 16x16 Words》的论文，给出了一个响亮的回答：可以！他们提出的 **视觉 Transformer (Vision Transformer, ViT)**，成功地将标准 Transformer 架构直接应用于图像分类，并在大规模数据集上取得了超越顶级 CNN 的性能，彻底改变了计算机视觉领域的格局。\n\n\nViT 的核心思想，就是想办法将一张二维的图片，转化成一个一维的“词元”序列 (Sequence of Tokens)，然后将其直接输入给一个标准的 Transformer 编码器。\n\n这个过程主要分为三步：\n\n**1. 图像的“词元化” (Image Patching)**\n\n这是 ViT 最关键的一步。它不再像 CNN 那样用卷积核逐个像素地扫描，而是简单粗暴地将输入的图片（例如 224x224）切割成一系列不重叠的、固定大小的**小块 (Patches)**。\n\n例如，如果每个 Patch 的大小是 16x16 像素，那么一张 224x224 的图片就会被切割成 `(224/16) * (224/16) = 14 * 14 = 196` 个 Patches。\n\n![ViT Patching](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-10-08_at_4.28.22_PM_5x1rSjN.png)\n*图片来源: paperswithcode.com*\n\n接着，每一个 Patch 都会被“压平”成一个一维向量（例如，一个 16x16x3 的 Patch 会被压平成一个 768 维的向量），然后通过一个标准的线性变换，将其映射到模型期望的维度（例如 512 维）。\n\n至此，我们就成功地将一张图片，转换成了一个由 196 个“词元”组成的序列。**每一个“词元”，就代表了原始图片的一个“小块”**。\n\n**2. 位置编码 (Positional Encoding)**\n\n和我们在第十一章学到的一样，标准的 Transformer 架构本身并不包含任何关于序列顺序的信息。为了让模型知道每个 Patch 来自于图片的哪个位置，ViT 同样为每一个 Patch 向量添加了一个可学习的**位置编码 (Positional Encoding)**。\n\n这个位置编码向量的作用，就是告诉模型每个“图像词元”的原始空间位置，使得模型能够理解“左上角”、“中心”、“右下角”等空间概念。\n\n**3. Transformer 编码器**\n\n经过以上两步处理后，我们得到了一系列的、包含了内容和位置信息的向量。这个向量序列，就可以被直接送入一个标准的 Transformer 编码器了。\n\n接下来的故事我们就很熟悉了：\n\n-   **多头自注意力机制 (Multi-Head Self-Attention)** 会在所有这些 Patch 之间计算注意力权重。这意味着，模型可以灵活地、动态地学习到图像中**任意两个 Patch 之间**的相互关系，无论它们相距多远。例如，它能直接关联左上角的一只猫耳朵和右下角的一条猫尾巴，形成对“猫”这个物体的全局理解。\n-   **前馈网络 (Feed-Forward Network)** 则负责对每个 Patch 的表示进行进一步的非线性变换。\n-   通过堆叠多个这样的编码器模块，ViT 能够学习到越来越丰富、越来越抽象的特征表示。\n\n最后，为了进行分类，ViT 在序列的最前面额外添加了一个特殊的、可学习的 `[CLS]` (Classification) 词元。在经过 Transformer 编码器之后，这个 `[CLS]` 词元最终的输出状态，就被认为是整张图片的全局特征表示。我们只需将这个向量送入一个简单的全连接层，就可以得到最终的分类结果。\n\n### CNN vs. ViT：两种哲学的碰撞\n\nCNN 和 ViT 代表了两种看待视觉问题的、截然不同的哲学。\n\n| 特性 | 卷积神经网络 (CNN) | 视觉 Transformer (ViT) |\n| :--- | :--- | :--- |\n| **核心思想** | 局部特征提取，层次化组合 | 全局关系建模 |\n| **归纳偏置** | **强偏置**：局部性、平移不变性被硬编码在架构中。 | **弱偏置**：除了位置编码，几乎没有内置的视觉先验知识。 |\n| **数据需求** | 数据效率高，在**中小规模**数据集上表现优异。 | 数据效率低，需要**大规模**数据集（如 ImageNet-21k, JFT-300M）进行预训练，才能学习到视觉的内在规律。 |\n| **优势** | 训练快，收敛稳定，对各种尺寸的数据集都表现良好。 | 在超大规模数据集上预训练后，其性能上限通常**高于** CNN，具有更好的扩展性 (Scalability)。 |\n| **类比** | 一位经验丰富的**工匠**，使用一套代代相传的、专门为木工活设计的工具（凿子、刨子），高效地制作家具。 | 一位手持通用、强大但没有预设功能的“万能工具”的**学习者**。如果只有少量木材，他可能无从下手；但如果给他一座森林，他能通过学习，最终造出比工匠更宏伟的建筑。 |\n\n::: {.callout-note title=\"架构师视角\"}\nCNN 和 ViT 的选择，是一个典型的**架构权衡 (Architectural Trade-off)**。\n\n-   如果你的项目面临**数据量有限**、需要**快速迭代**的场景，那么 CNN 及其丰富的预训练生态系统，通常是更稳健、更高效的选择。\n-   如果你有机会接触到**海量的专有数据**，并追求**极致的性能上限**，那么 ViT 强大的学习能力和扩展性，则可能为你带来更大的惊喜。\n\n在当今，许多最先进的模型（SOTA, State-of-the-art）都试图将两者结合起来，例如通过一个 CNN 来提取底层的局部特征，再将其输入给一个 Transformer 来建模全局关系，以期获得两种哲学的共同优势。\n:::\n","srcMarkdownNoYaml":"\n\n在 CNN 统治了计算机视觉领域近十年之后，一个来自自然语言处理领域的“跨界者”——Transformer，提出了一个颠覆性的问题：**我们能否像处理一个句子一样，来处理一张图片？**\n\n这个问题在最初听起来有些匪夷所思。CNN 的成功，建立在它为视觉任务精心设计的**归纳偏置 (Inductive Bias)** 之上，例如局部性 (Locality) 和平移不变性 (Translation Invariance)。这些偏置就像是模型在学习前就已经具备的“先验知识”，非常符合我们对视觉世界的直觉。而 Transformer，作为一个为处理序列数据而生的架构，似乎缺少这些专门为图像定制的“天赋”。\n\n然而，在2020年，Google 的研究者们用一篇名为《An Image is Worth 16x16 Words》的论文，给出了一个响亮的回答：可以！他们提出的 **视觉 Transformer (Vision Transformer, ViT)**，成功地将标准 Transformer 架构直接应用于图像分类，并在大规模数据集上取得了超越顶级 CNN 的性能，彻底改变了计算机视觉领域的格局。\n\n### ViT 的核心思想：把图片当成句子\n\nViT 的核心思想，就是想办法将一张二维的图片，转化成一个一维的“词元”序列 (Sequence of Tokens)，然后将其直接输入给一个标准的 Transformer 编码器。\n\n这个过程主要分为三步：\n\n**1. 图像的“词元化” (Image Patching)**\n\n这是 ViT 最关键的一步。它不再像 CNN 那样用卷积核逐个像素地扫描，而是简单粗暴地将输入的图片（例如 224x224）切割成一系列不重叠的、固定大小的**小块 (Patches)**。\n\n例如，如果每个 Patch 的大小是 16x16 像素，那么一张 224x224 的图片就会被切割成 `(224/16) * (224/16) = 14 * 14 = 196` 个 Patches。\n\n![ViT Patching](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-10-08_at_4.28.22_PM_5x1rSjN.png)\n*图片来源: paperswithcode.com*\n\n接着，每一个 Patch 都会被“压平”成一个一维向量（例如，一个 16x16x3 的 Patch 会被压平成一个 768 维的向量），然后通过一个标准的线性变换，将其映射到模型期望的维度（例如 512 维）。\n\n至此，我们就成功地将一张图片，转换成了一个由 196 个“词元”组成的序列。**每一个“词元”，就代表了原始图片的一个“小块”**。\n\n**2. 位置编码 (Positional Encoding)**\n\n和我们在第十一章学到的一样，标准的 Transformer 架构本身并不包含任何关于序列顺序的信息。为了让模型知道每个 Patch 来自于图片的哪个位置，ViT 同样为每一个 Patch 向量添加了一个可学习的**位置编码 (Positional Encoding)**。\n\n这个位置编码向量的作用，就是告诉模型每个“图像词元”的原始空间位置，使得模型能够理解“左上角”、“中心”、“右下角”等空间概念。\n\n**3. Transformer 编码器**\n\n经过以上两步处理后，我们得到了一系列的、包含了内容和位置信息的向量。这个向量序列，就可以被直接送入一个标准的 Transformer 编码器了。\n\n接下来的故事我们就很熟悉了：\n\n-   **多头自注意力机制 (Multi-Head Self-Attention)** 会在所有这些 Patch 之间计算注意力权重。这意味着，模型可以灵活地、动态地学习到图像中**任意两个 Patch 之间**的相互关系，无论它们相距多远。例如，它能直接关联左上角的一只猫耳朵和右下角的一条猫尾巴，形成对“猫”这个物体的全局理解。\n-   **前馈网络 (Feed-Forward Network)** 则负责对每个 Patch 的表示进行进一步的非线性变换。\n-   通过堆叠多个这样的编码器模块，ViT 能够学习到越来越丰富、越来越抽象的特征表示。\n\n最后，为了进行分类，ViT 在序列的最前面额外添加了一个特殊的、可学习的 `[CLS]` (Classification) 词元。在经过 Transformer 编码器之后，这个 `[CLS]` 词元最终的输出状态，就被认为是整张图片的全局特征表示。我们只需将这个向量送入一个简单的全连接层，就可以得到最终的分类结果。\n\n### CNN vs. ViT：两种哲学的碰撞\n\nCNN 和 ViT 代表了两种看待视觉问题的、截然不同的哲学。\n\n| 特性 | 卷积神经网络 (CNN) | 视觉 Transformer (ViT) |\n| :--- | :--- | :--- |\n| **核心思想** | 局部特征提取，层次化组合 | 全局关系建模 |\n| **归纳偏置** | **强偏置**：局部性、平移不变性被硬编码在架构中。 | **弱偏置**：除了位置编码，几乎没有内置的视觉先验知识。 |\n| **数据需求** | 数据效率高，在**中小规模**数据集上表现优异。 | 数据效率低，需要**大规模**数据集（如 ImageNet-21k, JFT-300M）进行预训练，才能学习到视觉的内在规律。 |\n| **优势** | 训练快，收敛稳定，对各种尺寸的数据集都表现良好。 | 在超大规模数据集上预训练后，其性能上限通常**高于** CNN，具有更好的扩展性 (Scalability)。 |\n| **类比** | 一位经验丰富的**工匠**，使用一套代代相传的、专门为木工活设计的工具（凿子、刨子），高效地制作家具。 | 一位手持通用、强大但没有预设功能的“万能工具”的**学习者**。如果只有少量木材，他可能无从下手；但如果给他一座森林，他能通过学习，最终造出比工匠更宏伟的建筑。 |\n\n::: {.callout-note title=\"架构师视角\"}\nCNN 和 ViT 的选择，是一个典型的**架构权衡 (Architectural Trade-off)**。\n\n-   如果你的项目面临**数据量有限**、需要**快速迭代**的场景，那么 CNN 及其丰富的预训练生态系统，通常是更稳健、更高效的选择。\n-   如果你有机会接触到**海量的专有数据**，并追求**极致的性能上限**，那么 ViT 强大的学习能力和扩展性，则可能为你带来更大的惊喜。\n\n在当今，许多最先进的模型（SOTA, State-of-the-art）都试图将两者结合起来，例如通过一个 CNN 来提取底层的局部特征，再将其输入给一个 Transformer 来建模全局关系，以期获得两种哲学的共同优势。\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":false,"number-sections":false,"highlight-style":"github","include-in-header":{"text":"<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap\" rel=\"stylesheet\">\n<style>\n/* ChatGPT 风格变量 */\n:root {\n  --chatgpt-primary: #000000;\n  --chatgpt-secondary: #6b7280;\n  --chatgpt-background: #ffffff;\n  --chatgpt-surface: #f7f7f8;\n  --chatgpt-border: #e5e5e5;\n  --chatgpt-accent: #10a37f;\n}\n</style>\n"},"output-file":"12_3_vit.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.14","theme":["cosmo","../assets/chatgpt-style.scss"],"fig-cap-location":"bottom","mainfont":"Inter, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Ubuntu, Cantarell, 'Noto Sans', sans-serif, 'Helvetica Neue', Arial, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'","monofont":"Monaco, 'SF Mono', 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace","fontsize":"16px","linestretch":1.6,"code-copy":true,"max-width":"1200px","title":"12.3 视觉 Transformer (ViT)：当 Transformer 开始“看”世界"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}