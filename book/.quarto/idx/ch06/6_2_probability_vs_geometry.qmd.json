{"title":"6.2 两种基础思路：概率 vs. 几何","markdown":{"yaml":{"title":"6.2 两种基础思路：概率 vs. 几何"},"headingText":"逻辑回归 (Logistic Regression)：概率视角","containsRefs":false,"markdown":"\n\n在定义了分类任务是“寻找决策边界”之后，一个自然的问题是：**如何找到最佳的决策边界？** 机器学习的历史上，针对这个问题演化出了许多不同的流派。本节，我们聚焦于两种最基础、思想也最迥异的思路：一种基于概率，另一种基于几何。\n\n\n你可能会对它的名字感到困惑：为什么一个叫“回归”的模型，却用来解决“分类”问题？\n\n这是因为逻辑回归的核心思想确实与线性回归一脉相承。它首先也像线性回归一样，将所有输入特征进行加权求和，得到一个预测值 `z`。\n$$ z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b $$\n但它并没有就此打住。它知道，对于分类问题，我们真正想要的不是一个可以无限延伸的 `z` 值，而是一个介于 0 和 1 之间的**概率**。\n\n为了实现这个转换，逻辑回归引入了一个至关重要的数学工具：**Sigmoid 函数**（也称作 Logistic 函数）。\n$$ \\text{Probability} = \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n这个函数无论输入 `z` 是多大或多小的数，都能将其“压扁”到 (0, 1) 的区间内，形成一条优美的“S”形曲线。\n\n<iframe src=\"../assets/ch06/sigmoid_curve.html\" width=\"100%\" height=\"480\" frameborder=\"0\"></iframe>\n\n*上图展示了Sigmoid函数的功能。当线性预测值 `z` 大于0时，输出概率就大于0.5；当 `z` 小于0时，概率就小于0.5。*\n\n**第一性原理与直觉**\n逻辑回归的本质不是直接预测“是”或“否”，而是**预测“是”的概率有多大**。它回答的是一个更微妙的问题：“根据该客户的特征，他有多大的可能性会流失？”\n\n这种基于概率的思考方式非常强大，因为它不仅给出了分类结果，还给出了这个结果的**置信度**。在商业决策中，知道一个客户有99%的可能会流失，和知道他有51%的可能会流失，我们可能会采取截然不同的挽留策略。\n\n### 支持向量机 (Support Vector Machine, SVM)：几何视角\n\n支持向量机则完全抛弃了概率的想法，它是一个纯粹的“几何学家”。它的目标简单而明确：**在特征空间中，找到那条能以最大“间隔”将两类数据点分开的决策边界。**\n\n**核心思想：最大化间隔 (Margin)**\n想象一下在两类数据点之间，我们要画一条分界线。我们可以画出无数条线将它们分开。但哪一条是最好的呢？SVM认为，最好的那条线，是离两边数据点都**最远**的那条线。这条线为未来的新数据点预留了最大的“容错空间”。\n\n这个“缓冲区”的宽度，就被称为**间隔 (Margin)**。而那些离决策边界最近的、定义了这个间隔边界的数据点，就被称为**支持向量 (Support Vectors)**。\n\n#### 决策函数与分类规则\n\n为了理解这些线在数学上是什么，我们需要引入SVM的**决策函数 (Decision Function)**，我们称之为 `f(x)`。对于一个线性的SVM，它和线性回归的形式非常相似：\n$$ f(x) = w \\cdot x + b = w_1 x_1 + w_2 x_2 + ... + b $$\n这个函数输出一个数值，这个数值代表了数据点 `x` 到决策边界的“有符号的距离”（经过了缩放）。\n\nSVM的分类规则极其简单：\n\n-   如果 `f(x) > 0`，则预测为**正类** (比如“不流失”)。\n-   如果 `f(x) < 0`，则预测为**负类** (比如“会流失”)。\n\n那么，我们前面提到的三条关键的线就有了明确的数学定义：\n\n1.  **决策边界**：就是所有满足 `f(x) = 0` 的点构成的线（或面）。\n2.  **正类间隔线**：就是所有满足 `f(x) = 1` 的点构成的线。它穿过离决策边界最近的正类支持向量。\n3.  **负类间隔线**：就是所有满足 `f(x) = -1` 的点构成的线。它穿过离决策边界最近的负类支持向量。\n\n所以，SVM的目标不仅是找到 `f(x)=0` 这条分界线，更是要让 `f(x)=1` 和 `f(x)=-1` 这两条间隔线之间的“街道”尽可能宽。\n\n<iframe src=\"../assets/ch06/svm_margin.html\" width=\"100%\" height=\"550\" frameborder=\"0\"></iframe>\n\n*在这个交互式动画中，你可以看到不同的决策边界（橙色虚线）会产生不同的间隔。SVM的目标就是找到那条能让间隔（灰色区域的宽度）最大的实线边界。注意，只有那几个最靠近边界的“支持向量”（带有黑圈的点）才决定了最终的决策边界，其他点则没有影响。*\n\n**物理类比**\n如果觉得这个概念还是有点抽象，可以想象一个物理场景：\n\n- 两类数据点是钉在墙上的两种不同颜色的**钉子**。\n- 我们要在它们之间放入一个**可以膨胀的通道**（比如一根粗壮的橡皮管）。\n- 我们让这个通道尽可能地膨胀，直到它被两边最近的几颗钉子“卡住”为止。\n- 这个通道最胖时的**中心线**，就是SVM找到的最佳决策边界。\n\n\n#### 硬间隔 vs. 软间隔：正则化参数 C 的权衡\n\n上面的动画展示的是一个理想情况，数据点被完美地分开了。但在现实世界中，数据往往是嘈杂的，可能无法被一条直线完美分割。\n\n这时，SVM引入了一个非常重要的**正则化参数 `C`**，它允许我们在“最大化间隔”和“最小化分类错误”之间做出权衡。\n\n-   **高 `C` 值 (Hard Margin)**: `C` 很大时，意味着对误分类的惩罚极高。SVM会变得非常“严格”，试图找到一个能正确分类所有（或绝大多数）数据点的决策边界，即使这意味着间隔会变得非常窄。这可能导致模型对训练数据中的噪声非常敏感，容易**过拟合**。\n-   **低 `C` 值 (Soft Margin)**: `C` 很小时，意味着我们对误分类的容忍度更高。SVM会更专注于寻找一个更宽的间隔，即使这会导致一些数据点被分错。这通常能让模型具有更好的**泛化能力**，对新数据的表现更好。\n\n这个 `C` 参数正是架构师在训练SVM模型时需要精细调节的核心超参数之一。它体现了在复杂现实和简洁模型之间的经典权衡。\n\n**与下图互动，亲手感受 `C` 的威力：**\n\n请仔细观察下面的交互式动画。它清晰地展示了正则化参数 `C` 如何在“最大化间隔”与“最小化分类错误”这两个目标之间进行权衡。\n\n<iframe src=\"../assets/ch06/svm_hard_soft_margin_C.html\" width=\"100%\" height=\"700\" frameborder=\"0\"></iframe>\n\n**探索与思考：**\n\n1.  **拖动 `C` 的滑块从左到右**：\n    -   **当 `C` 很小（软间隔）时**：注意看，模型容忍了几个被标为“×”的误分类点，但换来了一个非常宽的灰色间隔区域。这代表模型更注重整体的泛化能力。\n    -   **当 `C` 增大（硬间隔）时**：模型变得越来越“严格”，灰色间隔不断收缩，以确保将尽可能多的点正确分类。当 `C` 非常大时，模型甚至会为了迁就个别的“离群点”而使间隔变得极窄，这正是过拟合风险的体现。\n\n2.  **思考业务成本**：\n    -   想象一下，如果这是一个**“癌症诊断”**模型，将一个恶性肿瘤（正类）误判为良性（负类）的代价极高。在这种场景下，你倾向于选择一个更大的 `C` 还是更小的 `C`？为什么？\n    -   如果这是一个**“产品推荐”**模型，偶尔推荐错一个用户可能不感兴趣的产品，代价相对较低。这时，你的选择又会是什么？\n\n通过这个简单的参数 `C`，SVM赋予了我们作为“架构师”一个强大的调控旋钮，让我们能够根据具体的业务需求，在模型的复杂度和泛化能力之间找到最佳的平衡点。\n\n**总结与对比**\n\n| 特性 | 逻辑回归 | 支持向量机 |\n| :--- | :--- | :--- |\n| **核心思想** | 概率拟合 (拟合S形曲线) | 几何分割 (最大化间隔) |\n| **输出** | 属于某个类别的概率 (0-1) | 直接的类别判断 (属于哪一边) |\n| **关注点** | 关注所有数据点 | 只关注最靠近边界的支持向量 |\n| **可解释性** | 较好，输出的概率有业务意义 | 较差，决策边界的权重不易直观解释 |\n\n这两种思路代表了分类问题中两种不同的哲学。逻辑回归更“圆滑”，它给出了一个带有不确定性的概率判断；而SVM则更“极致”，它追求的是最稳健、最安全的几何划分。\n","srcMarkdownNoYaml":"\n\n在定义了分类任务是“寻找决策边界”之后，一个自然的问题是：**如何找到最佳的决策边界？** 机器学习的历史上，针对这个问题演化出了许多不同的流派。本节，我们聚焦于两种最基础、思想也最迥异的思路：一种基于概率，另一种基于几何。\n\n### 逻辑回归 (Logistic Regression)：概率视角\n\n你可能会对它的名字感到困惑：为什么一个叫“回归”的模型，却用来解决“分类”问题？\n\n这是因为逻辑回归的核心思想确实与线性回归一脉相承。它首先也像线性回归一样，将所有输入特征进行加权求和，得到一个预测值 `z`。\n$$ z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b $$\n但它并没有就此打住。它知道，对于分类问题，我们真正想要的不是一个可以无限延伸的 `z` 值，而是一个介于 0 和 1 之间的**概率**。\n\n为了实现这个转换，逻辑回归引入了一个至关重要的数学工具：**Sigmoid 函数**（也称作 Logistic 函数）。\n$$ \\text{Probability} = \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n这个函数无论输入 `z` 是多大或多小的数，都能将其“压扁”到 (0, 1) 的区间内，形成一条优美的“S”形曲线。\n\n<iframe src=\"../assets/ch06/sigmoid_curve.html\" width=\"100%\" height=\"480\" frameborder=\"0\"></iframe>\n\n*上图展示了Sigmoid函数的功能。当线性预测值 `z` 大于0时，输出概率就大于0.5；当 `z` 小于0时，概率就小于0.5。*\n\n**第一性原理与直觉**\n逻辑回归的本质不是直接预测“是”或“否”，而是**预测“是”的概率有多大**。它回答的是一个更微妙的问题：“根据该客户的特征，他有多大的可能性会流失？”\n\n这种基于概率的思考方式非常强大，因为它不仅给出了分类结果，还给出了这个结果的**置信度**。在商业决策中，知道一个客户有99%的可能会流失，和知道他有51%的可能会流失，我们可能会采取截然不同的挽留策略。\n\n### 支持向量机 (Support Vector Machine, SVM)：几何视角\n\n支持向量机则完全抛弃了概率的想法，它是一个纯粹的“几何学家”。它的目标简单而明确：**在特征空间中，找到那条能以最大“间隔”将两类数据点分开的决策边界。**\n\n**核心思想：最大化间隔 (Margin)**\n想象一下在两类数据点之间，我们要画一条分界线。我们可以画出无数条线将它们分开。但哪一条是最好的呢？SVM认为，最好的那条线，是离两边数据点都**最远**的那条线。这条线为未来的新数据点预留了最大的“容错空间”。\n\n这个“缓冲区”的宽度，就被称为**间隔 (Margin)**。而那些离决策边界最近的、定义了这个间隔边界的数据点，就被称为**支持向量 (Support Vectors)**。\n\n#### 决策函数与分类规则\n\n为了理解这些线在数学上是什么，我们需要引入SVM的**决策函数 (Decision Function)**，我们称之为 `f(x)`。对于一个线性的SVM，它和线性回归的形式非常相似：\n$$ f(x) = w \\cdot x + b = w_1 x_1 + w_2 x_2 + ... + b $$\n这个函数输出一个数值，这个数值代表了数据点 `x` 到决策边界的“有符号的距离”（经过了缩放）。\n\nSVM的分类规则极其简单：\n\n-   如果 `f(x) > 0`，则预测为**正类** (比如“不流失”)。\n-   如果 `f(x) < 0`，则预测为**负类** (比如“会流失”)。\n\n那么，我们前面提到的三条关键的线就有了明确的数学定义：\n\n1.  **决策边界**：就是所有满足 `f(x) = 0` 的点构成的线（或面）。\n2.  **正类间隔线**：就是所有满足 `f(x) = 1` 的点构成的线。它穿过离决策边界最近的正类支持向量。\n3.  **负类间隔线**：就是所有满足 `f(x) = -1` 的点构成的线。它穿过离决策边界最近的负类支持向量。\n\n所以，SVM的目标不仅是找到 `f(x)=0` 这条分界线，更是要让 `f(x)=1` 和 `f(x)=-1` 这两条间隔线之间的“街道”尽可能宽。\n\n<iframe src=\"../assets/ch06/svm_margin.html\" width=\"100%\" height=\"550\" frameborder=\"0\"></iframe>\n\n*在这个交互式动画中，你可以看到不同的决策边界（橙色虚线）会产生不同的间隔。SVM的目标就是找到那条能让间隔（灰色区域的宽度）最大的实线边界。注意，只有那几个最靠近边界的“支持向量”（带有黑圈的点）才决定了最终的决策边界，其他点则没有影响。*\n\n**物理类比**\n如果觉得这个概念还是有点抽象，可以想象一个物理场景：\n\n- 两类数据点是钉在墙上的两种不同颜色的**钉子**。\n- 我们要在它们之间放入一个**可以膨胀的通道**（比如一根粗壮的橡皮管）。\n- 我们让这个通道尽可能地膨胀，直到它被两边最近的几颗钉子“卡住”为止。\n- 这个通道最胖时的**中心线**，就是SVM找到的最佳决策边界。\n\n\n#### 硬间隔 vs. 软间隔：正则化参数 C 的权衡\n\n上面的动画展示的是一个理想情况，数据点被完美地分开了。但在现实世界中，数据往往是嘈杂的，可能无法被一条直线完美分割。\n\n这时，SVM引入了一个非常重要的**正则化参数 `C`**，它允许我们在“最大化间隔”和“最小化分类错误”之间做出权衡。\n\n-   **高 `C` 值 (Hard Margin)**: `C` 很大时，意味着对误分类的惩罚极高。SVM会变得非常“严格”，试图找到一个能正确分类所有（或绝大多数）数据点的决策边界，即使这意味着间隔会变得非常窄。这可能导致模型对训练数据中的噪声非常敏感，容易**过拟合**。\n-   **低 `C` 值 (Soft Margin)**: `C` 很小时，意味着我们对误分类的容忍度更高。SVM会更专注于寻找一个更宽的间隔，即使这会导致一些数据点被分错。这通常能让模型具有更好的**泛化能力**，对新数据的表现更好。\n\n这个 `C` 参数正是架构师在训练SVM模型时需要精细调节的核心超参数之一。它体现了在复杂现实和简洁模型之间的经典权衡。\n\n**与下图互动，亲手感受 `C` 的威力：**\n\n请仔细观察下面的交互式动画。它清晰地展示了正则化参数 `C` 如何在“最大化间隔”与“最小化分类错误”这两个目标之间进行权衡。\n\n<iframe src=\"../assets/ch06/svm_hard_soft_margin_C.html\" width=\"100%\" height=\"700\" frameborder=\"0\"></iframe>\n\n**探索与思考：**\n\n1.  **拖动 `C` 的滑块从左到右**：\n    -   **当 `C` 很小（软间隔）时**：注意看，模型容忍了几个被标为“×”的误分类点，但换来了一个非常宽的灰色间隔区域。这代表模型更注重整体的泛化能力。\n    -   **当 `C` 增大（硬间隔）时**：模型变得越来越“严格”，灰色间隔不断收缩，以确保将尽可能多的点正确分类。当 `C` 非常大时，模型甚至会为了迁就个别的“离群点”而使间隔变得极窄，这正是过拟合风险的体现。\n\n2.  **思考业务成本**：\n    -   想象一下，如果这是一个**“癌症诊断”**模型，将一个恶性肿瘤（正类）误判为良性（负类）的代价极高。在这种场景下，你倾向于选择一个更大的 `C` 还是更小的 `C`？为什么？\n    -   如果这是一个**“产品推荐”**模型，偶尔推荐错一个用户可能不感兴趣的产品，代价相对较低。这时，你的选择又会是什么？\n\n通过这个简单的参数 `C`，SVM赋予了我们作为“架构师”一个强大的调控旋钮，让我们能够根据具体的业务需求，在模型的复杂度和泛化能力之间找到最佳的平衡点。\n\n**总结与对比**\n\n| 特性 | 逻辑回归 | 支持向量机 |\n| :--- | :--- | :--- |\n| **核心思想** | 概率拟合 (拟合S形曲线) | 几何分割 (最大化间隔) |\n| **输出** | 属于某个类别的概率 (0-1) | 直接的类别判断 (属于哪一边) |\n| **关注点** | 关注所有数据点 | 只关注最靠近边界的支持向量 |\n| **可解释性** | 较好，输出的概率有业务意义 | 较差，决策边界的权重不易直观解释 |\n\n这两种思路代表了分类问题中两种不同的哲学。逻辑回归更“圆滑”，它给出了一个带有不确定性的概率判断；而SVM则更“极致”，它追求的是最稳健、最安全的几何划分。\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":false,"number-sections":false,"output-file":"6_2_probability_vs_geometry.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.14","theme":"cosmo","fig-cap-location":"bottom","title":"6.2 两种基础思路：概率 vs. 几何"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}