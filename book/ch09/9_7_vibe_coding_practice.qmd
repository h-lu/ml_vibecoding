---
title: "9.7 Vibe Coding 实践：拟合非线性边界"
---

欢迎来到第九章的 Vibe Coding 实践！在本章中，我们从理论上学习了如何从单个神经元构建出强大的全连接网络，并了解了驱动它学习的“引擎”和让它变得更“深”的“神级装备”。

现在，是时候将理论付诸实践了。我们的目标非常明确：**回到 `9.1` 节的那个线性模型无法解决的“月牙”分类问题，并使用我们新学的全连接网络（FCN）来完美地解决它。**

通过这次实践，你将亲身体会到非线性激活函数和多层结构所带来的强大能力。

### 我们的目标
- **构建你的第一个神经网络**：使用 PyTorch 或 TensorFlow，搭建一个简单的全连接网络。
- **可视化决策边界**：将模型在训练过程中的决策边界动态地绘制出来，直观地感受模型是如何从一条直线，逐渐“扭曲”成能够拟合月牙形状的复杂曲线的。
- **探索超参数**：通过调整网络结构（层数、神经元数量）、激活函数和正则化技术，理解它们对模型学习过程和最终结果的影响。


### 第一阶段：AI 快速搭建 FCN 并实现可视化

我们的核心任务是：创建一个可以对 `scikit-learn` 中的 `make_moons` 数据集进行分类的神经网络，并在训练的每个阶段都将其决策边界可视化。

::: {.callout-important title="Vibe Coding 提示"}
**向你的 AI 助手发出指令：**

“我需要使用 PyTorch 和 scikit-learn 来构建一个神经网络，对 `make_moons` 数据集进行分类，并可视化其训练过程。请帮我完成以下步骤：

1.  **生成并准备数据**：使用 `sklearn.datasets.make_moons` 生成200个样本，噪声设置为0.2。将数据转换为 PyTorch 的 Tensor。
2.  **定义网络架构**：
    -   创建一个继承自 `torch.nn.Module` 的类来定义你的全连接网络。
    -   网络结构为：输入层 (2个神经元) -> 隐藏层1 (16个神经元) -> ReLU激活 -> 隐藏层2 (16个神经元) -> ReLU激活 -> 输出层 (1个神经元) -> Sigmoid激活。
3.  **定义损失与优化器**：
    -   使用 `torch.nn.BCELoss` (二元交叉熵损失)作为损失函数。
    -   使用 `torch.optim.Adam` 作为优化器，学习率设置为 0.01。
4.  **训练循环**：
    -   编写一个训练循环，迭代 1000 次。
    -   在循环的每个步骤中，执行前向传播、计算损失、反向传播和更新权重。
5.  **可视化决策边界**：
    -   为了可视化，你需要一个辅助函数。这个函数会创建一个网格 (mesh grid)，然后用当前训练好的模型对网格上每个点的类别进行预测。
    -   使用 `matplotlib.pyplot.contourf` 函数，根据模型的预测结果来绘制决策边界的背景色。
    -   在训练循环中，每隔100次迭代，就调用这个可视化函数，绘制当前的决策边界，并将原始的 `make_moons` 数据点叠加在上面。可以使用 `plt.pause()` 来创建动态效果，或者将图片保存下来。

请确保代码是完整且可以直接运行的。”
:::

*架构师的思考：这个 Prompt 非常清晰地将一个复杂的任务分解为了五个可执行的步骤。我们为 AI 提供了明确的网络结构、损失函数、优化器等关键信息，让它可以心无旁骛地聚焦于代码生成。同时，可视化的要求是这次实践的核心，它能将抽象的训练过程转化为直观的视觉反馈。*



### 第二阶段：人类引导、探索与优化

AI 已经为我们生成了可以工作的代码，并展示了模型学习的过程。现在，真正有趣的探索开始了。你应该能看到，决策边界从一条近乎直线，慢慢地变得越来越弯曲，最终严丝合缝地包裹住了月牙数据。

**请你和你的学习小组，围绕以下问题进行探索和思考：**

1.  **激活函数是关键吗？**
    -   **动手修改**：回到你定义的网络模型中，将所有的 `ReLU` 激活函数**注释掉或移除**，换成线性的占位符（或者干脆什么都不加）。重新运行代码。
    -   **观察与思考**：你看到了什么？决策边界还能变弯曲吗？为什么会这样？这个实验如何用最直观的方式证明了“非线性激活”是神经网络能够学习复杂模式的**根本原因**？

2.  **网络深度/宽度的影响？**
    -   **动手修改**：
        -   **变浅变窄**：尝试将网络改为只有1个隐藏层，8个神经元。它的决策边界是什么样的？能完美拟合吗？
        -   **变深变宽**：尝试增加到3个隐藏层，每层32个神经元。决策边界是变得更平滑了，还是更“过拟合”（出现一些奇怪的、不必要的弯曲）了？
    -   **思考**：网络的深度和宽度，如何影响它拟合复杂函数的能力和过拟合的风险？

3.  **正则化的力量**
    -   **制造过拟合**：首先，将 `make_moons` 的噪声 `noise` 参数调高，比如到 `0.4`，让数据点的重叠更严重。然后，使用一个非常宽、非常深的网络（例如，4层，每层64个神经元）去训练它。你应该能观察到非常明显的过拟合现象——决策边界变得极其扭曲，试图将每一个嘈杂的点都分开。
    -   **引入 Dropout**：现在，在你的网络模型的每个隐藏层后面，都加入一个 `torch.nn.Dropout` 层，p 值可以设置为 0.3。重新训练模型。
    -   **观察与思考**：加入 Dropout 后，决策边界发生了什么变化？它是不是变得更平滑、更“合理”了？Dropout 是如何帮助模型抵抗噪声、防止过拟合的？

通过以上实践，你不仅亲手构建并训练了你的第一个“深度”神经网络，更重要的是，你通过动手实验，深刻地理解了那些在理论课上听起来有些抽象的概念——非线性、网络容量、正则化——在实践中究竟意味着什么。这正是从“知道”到“理解”的关键一步。
