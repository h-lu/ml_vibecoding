---
title: "8.3 超越 K-means：从密度、层次到概率的视角"
---

K-means 算法简洁而高效，但它的两大局限——需要预设 K 值和只能处理球形簇——在许多真实场景中成为了它的“阿喀琉斯之踵”。为了克服这些限制，数据科学家们从不同的第一性原理视角出发，发展出了一系列更强大、更灵活的聚类算法。

本节，我们将探索其中三种最重要的思想：基于**密度**的 DBSCAN，基于**层次**的 Agglomerative Clustering，以及基于**概率**的高斯混合模型。

### DBSCAN：基于密度的视角

**核心思想**：一个簇是数据空间中一个连续的**“高密度区域”**。

想象一下夜晚的星空，我们很自然地会将那些密集地聚集在一起的星星看作一个“星座”（簇），而那些零散、孤立的星星则被我们忽略（噪声）。DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 正是模仿了这种人类的直觉。

它不关心簇的形状是什么样的，只关心数据点之间的“连接性”和“稠密程度”。它的工作方式由两个核心参数定义：

1.  **`eps` (ε)**：一个距离阈值，用于定义一个点的“邻域”范围。
2.  **`min_samples`**：一个整数，用于定义一个点的“邻域”内至少需要有多少个其他点，才能称之为一个“高密度”的核心点 (Core Point)。

**算法直觉**：

1.  从任意一个点开始，检查它的 `eps`-邻域内是否有足够多的点（≥ `min_samples`）。
2.  如果有，它就是一个**核心点**，一个新的簇就此诞生。然后，算法会像“滚雪球”一样，将这个核心点邻域内的所有点（包括其他核心点和非核心的边缘点）都拉入这个簇。这个过程会不断地扩展，直到这个高密度区域的边界被找到。
3.  如果一个点不是核心点，且不属于任何一个簇的边界，那么它就被标记为**噪声 (Noise)**。

<iframe src="../assets/ch08/dbscan_live.html" width=700" height="800" frameborder="0"></iframe>

**优势**：

-   **能发现任意形状的簇**：由于只关心连接性，DBSCAN 可以轻松地发现环形、条状等非球形簇。
-   **能识别噪声点**：这是 K-means 完全不具备的能力。在数据清洗、异常检测等领域非常有用。
-   **无需预先指定簇的数量**：簇的数量是由数据本身的密度分布决定的。

### 层次聚类：自下而上或自上而下的视角

**核心思想**：它不直接给出一个“最终”的聚类结果，而是创建出一系列**嵌套的簇**，并用一个**树状图 (Dendrogram)** 来展示它们之间的层次关系。

最常见的层次聚类策略是**凝聚式 (Agglomerative)**，它的工作流程非常符合直觉：

1.  **初始状态**：将每一个数据点都看作一个独立的簇。
2.  **迭代合并**：找到所有簇中“距离”最近的两个簇，将它们合并成一个新的簇。
3.  **循环往复**：重复步骤 2，直到所有的数据点都被合并到一个唯一的、巨大的簇中。

这里的关键在于如何定义两个簇之间的“距离”，常见的“链接” (Linkage) 方法有：

-   **Single Linkage**：取两个簇中**最近**的两个点之间的距离。
-   **Complete Linkage**：取两个簇中**最远**的两个点之间的距离。
-   **Average Linkage**：取两个簇中所有点对距离的**平均值**。
-   **Ward's Linkage**：计算合并两个簇后，总的簇内平方和会增加多少。它倾向于合并那些能让“总能量”增加最少的簇，因此通常能得到比较均匀的簇。

<iframe src="../assets/ch08/hierarchical_clustering_dendrogram.html" width="700" height="750" frameborder="0"></iframe>

**优势**：

-   **无需预设 K 值**：我们可以先生成完整的树状图，然后通过观察树的结构，决定在哪个“高度”进行“切割”，从而得到我们想要的任意数量的簇。这使得我们能理解数据在不同粒度下的群体结构。

### 高斯混合模型 (GMM)：基于概率的视角

**核心思想**：假设所有的数据点来自于 K 个不同的**高斯分布（正态分布）**的混合体。每一个高斯分布就代表一个簇。

K-means 的分配是**硬分配 (Hard Assignment)**，即每个点 100% 地属于某一个簇。但现实世界是模糊的，尤其是在簇的边界区域。GMM 则提供了一种更灵活、更符合现实的**软分配 (Soft Assignment)**。

GMM 的目标是，通过期望最大化 (EM) 算法，找到这 K 个高斯分布的最佳参数（均值、协方差、权重），使得这组分布能够**最大化地拟合（生成）我们观测到的数据**。

算法完成后，它不会直接告诉我们“这个点属于簇 A”，而是会给出一个**概率**：“这个点有 80% 的概率来自簇 A 的高斯分布，有 20% 的概率来自簇 B 的高斯分布”。

<iframe src="../assets/ch08/gmm_interactive.html" width="700" height="850" frameborder="0"></iframe>

**优势**：

-   **形状灵活**：由于每个高斯分布都有自己的协方差矩阵，GMM 能够捕捉到**椭圆形**的、甚至不同朝向的复杂簇结构，远比 K-means 的球形假设要灵活。
-   **软分配信息更丰富**：对于商业应用来说，知道一个客户有“70% 可能属于高价值群体，30% 可能属于中价值群体”，比强行给他打上“高价值”的标签，可能更有利于我们进行灰度、渐进的营销策略。
-   **基于概率，理论完备**：GMM 是一个生成模型，它有坚实的概率论基础，并能通过信息准则（如 AIC, BIC）来辅助判断最佳的簇数量 K。
