---
title: "8.2 K-means：寻找群体的“能量中心”"
---

K-means 是迄今为止最著名、最广泛使用的聚类算法。它的成功源于其思想的简洁与高效。从第一性原理来看，K-means 算法试图解决一个非常直观的几何问题。

### 几何直觉：最小化群体的“总能量”

想象一下，在一个二维平面上散落着一堆数据点。我们的任务是找到 K 个点，作为这 K 个群体的“中心”，然后将每个数据点都分配给离它最近的那个中心。

我们如何判断一组“中心”的摆放位置是好是坏呢？一个很自然的想法是：**好的聚类，其簇内成员应该紧密地团结在自己的中心周围。**

K-means 将这个想法量化为：**最小化所有数据点到其所属簇的质心 (Centroid) 的距离平方和**。这个“距离平方和”在物理学上被称为“转动惯量”，我们也可以将其通俗地理解为一个聚类系统的**“总能量”**。一个内部紧凑、稳定的系统，其总能量是最低的。

所以，K-means 的目标可以被形式化地描述为：
$$
\arg\min_{\mathbf{S}} \sum_{i=1}^{K} \sum_{\mathbf{x} \in S_i} \left\| \mathbf{x} - \boldsymbol{\mu}_i \right\|^2
$$
其中，$\boldsymbol{\mu}_i$ 是第 i 个簇 $S_i$ 的质心。

### 算法过程：一个不断寻求“稳态”的物理系统

要直接解决上面这个优化问题非常困难。幸运的是，我们可以通过一个非常优雅的迭代算法——**期望最大化 (Expectation-Maximization, EM)** 的思想来逐步逼近最优解。我们可以把这个过程类比为一个物理系统的演化：

1.  **初始化 (Initialization)**：随机地在数据空间中撒下 K 个“引力中心”（初始质心）。这些初始位置可能很糟糕，但没关系，系统会自动演化。

2.  **分配 (Assignment / Expectation Step)**：对于每一个数据点，计算它与 K 个引力中心的距离，然后将它“捕获”或分配给离它最近的那个引力中心。这一步完成后，我们就暂时形成了 K 个簇。

3.  **更新 (Update / Maximization Step)**：对于每一个引力中心，它会重新计算其所“捕获”到的所有数据点的几何中心（即均值），然后**移动到这个新的“重心”位置**。

4.  **迭代 (Iteration)**：重复执行第 2 步（根据新的中心重新分配所有点）和第 3 步（根据新的分配重新计算中心位置），直到引力中心的位置不再发生变化（或变化非常小），此时系统达到了一个**“能量稳定”**的状态，我们就找到了一个（可能是局部的）最优解。

### 互动演示：K-means 的动态演化


下图由**三幅子图**构成，**分别对应三组不同的随机种子（Seed）**。拖动底部的 **“选择 K” 滑块（2–6）**，三幅子图会**同时**更新到该 K 下的聚类结果。
每幅子图都标注了**质心**（黑色方块）和对应的 **Inertia**，便于横向比较。

<iframe src="../assets/ch08/kmeans_three_seeds_k_slider.html" width="100%" height="600" frameborder="0"></iframe>

**请与上图互动，并思考：**

1. **初始化的重要性（随机性）**
   对于同一个 K，**不同的随机种子**可能得到**不同的聚类结果**与 Inertia，这体现了 K-means 对初始化的敏感性（容易陷入局部最优）。`scikit-learn` 的 `init='k-means++'` 能**缓解但无法完全消除**这种随机性；实际建模时可用更大的 `n_init`（多次随机启动取最优）来稳健化结果。

2. **K 值的选择（模型复杂度）**
   拖动滑块观察 **K=2、3、4**：当 **K=3** 时，与数据的“自然”结构最契合；**K=2** 往往**合并**了本应分开的簇（欠拟合）；**K=4** 则可能把本来完整的簇**过度拆分**（过拟合）。另外注意 Inertia 会随 K 增大而下降，但**不能仅凭 Inertia 最小化来选 K**，常配合“肘部法”“轮廓系数”等指标综合判断。

<iframe src="../assets/ch08/kmeans_elbow_plot.html" width="100%" height="480" frameborder="0"></iframe>

### K-means 的阿喀琉斯之踵

通过上面的分析和互动，我们可以清晰地看到 K-means 的两个主要局限性：

1.  **K 值需要预先指定**：在大多数真实问题中，我们并不知道数据应该被分成几类。选择一个错误的 K 值，会导致毫无意义的聚类结果。
2.  **对形状敏感**：K-means 的内在假设是簇的形状是**球形**的（或更准确地说，是凸的），因为它用一个“中心”来代表整个簇。如果数据的真实簇结构是条状的、环形的或者其他不规则形状，K-means 的表现会非常糟糕。

这两个核心挑战，正是我们接下来要学习更高级聚类算法的根本原因。在下一节，我们将看到 DBSCAN 和层次聚类是如何从不同的视角来克服这些问题的。
