---
title: "6.2 两种基础思路：概率 vs. 几何"
---

在定义了分类任务是“寻找决策边界”之后，一个自然的问题是：**如何找到最佳的决策边界？** 机器学习的历史上，针对这个问题演化出了许多不同的流派。本节，我们聚焦于两种最基础、思想也最迥异的思路：一种基于概率，另一种基于几何。

### 逻辑回归 (Logistic Regression)：概率视角

你可能会对它的名字感到困惑：为什么一个叫“回归”的模型，却用来解决“分类”问题？

这是因为逻辑回归的核心思想确实与线性回归一脉相承。它首先也像线性回归一样，将所有输入特征进行加权求和，得到一个预测值 `z`。
$$ z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b $$
但它并没有就此打住。它知道，对于分类问题，我们真正想要的不是一个可以无限延伸的 `z` 值，而是一个介于 0 和 1 之间的**概率**。

为了实现这个转换，逻辑回归引入了一个至关重要的数学工具：**Sigmoid 函数**（也称作 Logistic 函数）。
$$ \text{Probability} = \sigma(z) = \frac{1}{1 + e^{-z}} $$
这个函数无论输入 `z` 是多大或多小的数，都能将其“压扁”到 (0, 1) 的区间内，形成一条优美的“S”形曲线。

<iframe src="../assets/ch06/sigmoid_curve.html" width="100%" height="480" frameborder="0"></iframe>

*上图展示了Sigmoid函数的功能。当线性预测值 `z` 大于0时，输出概率就大于0.5；当 `z` 小于0时，概率就小于0.5。*

**第一性原理与直觉**
逻辑回归的本质不是直接预测“是”或“否”，而是**预测“是”的概率有多大**。它回答的是一个更微妙的问题：“根据该客户的特征，他有多大的可能性会流失？”

这种基于概率的思考方式非常强大，因为它不仅给出了分类结果，还给出了这个结果的**置信度**。在商业决策中，知道一个客户有99%的可能会流失，和知道他有51%的可能会流失，我们可能会采取截然不同的挽留策略。

### 支持向量机 (Support Vector Machine, SVM)：几何视角

支持向量机则完全抛弃了概率的想法，它是一个纯粹的“几何学家”。它的目标简单而明确：**在特征空间中，找到那条能以最大“间隔”将两类数据点分开的决策边界。**

**核心思想：最大化间隔 (Margin)**
想象一下在两类数据点之间，我们要画一条分界线。我们可以画出无数条线将它们分开。但哪一条是最好的呢？SVM认为，最好的那条线，是离两边数据点都**最远**的那条线。这条线为未来的新数据点预留了最大的“容错空间”。

这个“缓冲区”的宽度，就被称为**间隔 (Margin)**。而那些离决策边界最近的、定义了这个间隔边界的数据点，就被称为**支持向量 (Support Vectors)**。

#### 决策函数与分类规则

为了理解这些线在数学上是什么，我们需要引入SVM的**决策函数 (Decision Function)**，我们称之为 `f(x)`。对于一个线性的SVM，它和线性回归的形式非常相似：
$$ f(x) = w \cdot x + b = w_1 x_1 + w_2 x_2 + ... + b $$
这个函数输出一个数值，这个数值代表了数据点 `x` 到决策边界的“有符号的距离”（经过了缩放）。

SVM的分类规则极其简单：

-   如果 `f(x) > 0`，则预测为**正类** (比如“不流失”)。
-   如果 `f(x) < 0`，则预测为**负类** (比如“会流失”)。

那么，我们前面提到的三条关键的线就有了明确的数学定义：

1.  **决策边界**：就是所有满足 `f(x) = 0` 的点构成的线（或面）。
2.  **正类间隔线**：就是所有满足 `f(x) = 1` 的点构成的线。它穿过离决策边界最近的正类支持向量。
3.  **负类间隔线**：就是所有满足 `f(x) = -1` 的点构成的线。它穿过离决策边界最近的负类支持向量。

所以，SVM的目标不仅是找到 `f(x)=0` 这条分界线，更是要让 `f(x)=1` 和 `f(x)=-1` 这两条间隔线之间的“街道”尽可能宽。

<iframe src="../assets/ch06/svm_margin.html" width="100%" height="550" frameborder="0"></iframe>

*在这个交互式动画中，你可以看到不同的决策边界（橙色虚线）会产生不同的间隔。SVM的目标就是找到那条能让间隔（灰色区域的宽度）最大的实线边界。注意，只有那几个最靠近边界的“支持向量”（带有黑圈的点）才决定了最终的决策边界，其他点则没有影响。*

**物理类比**
如果觉得这个概念还是有点抽象，可以想象一个物理场景：

- 两类数据点是钉在墙上的两种不同颜色的**钉子**。
- 我们要在它们之间放入一个**可以膨胀的通道**（比如一根粗壮的橡皮管）。
- 我们让这个通道尽可能地膨胀，直到它被两边最近的几颗钉子“卡住”为止。
- 这个通道最胖时的**中心线**，就是SVM找到的最佳决策边界。


#### 硬间隔 vs. 软间隔：正则化参数 C 的权衡

上面的动画展示的是一个理想情况，数据点被完美地分开了。但在现实世界中，数据往往是嘈杂的，可能无法被一条直线完美分割。

这时，SVM引入了一个非常重要的**正则化参数 `C`**，它允许我们在“最大化间隔”和“最小化分类错误”之间做出权衡。

-   **高 `C` 值 (Hard Margin)**: `C` 很大时，意味着对误分类的惩罚极高。SVM会变得非常“严格”，试图找到一个能正确分类所有（或绝大多数）数据点的决策边界，即使这意味着间隔会变得非常窄。这可能导致模型对训练数据中的噪声非常敏感，容易**过拟合**。
-   **低 `C` 值 (Soft Margin)**: `C` 很小时，意味着我们对误分类的容忍度更高。SVM会更专注于寻找一个更宽的间隔，即使这会导致一些数据点被分错。这通常能让模型具有更好的**泛化能力**，对新数据的表现更好。

这个 `C` 参数正是架构师在训练SVM模型时需要精细调节的核心超参数之一。它体现了在复杂现实和简洁模型之间的经典权衡。

**与下图互动，亲手感受 `C` 的威力：**

请仔细观察下面的交互式动画。它清晰地展示了正则化参数 `C` 如何在“最大化间隔”与“最小化分类错误”这两个目标之间进行权衡。

<iframe src="../assets/ch06/svm_hard_soft_margin_C.html" width="100%" height="700" frameborder="0"></iframe>

**探索与思考：**

1.  **拖动 `C` 的滑块从左到右**：
    -   **当 `C` 很小（软间隔）时**：注意看，模型容忍了几个被标为“×”的误分类点，但换来了一个非常宽的灰色间隔区域。这代表模型更注重整体的泛化能力。
    -   **当 `C` 增大（硬间隔）时**：模型变得越来越“严格”，灰色间隔不断收缩，以确保将尽可能多的点正确分类。当 `C` 非常大时，模型甚至会为了迁就个别的“离群点”而使间隔变得极窄，这正是过拟合风险的体现。

2.  **思考业务成本**：
    -   想象一下，如果这是一个**“癌症诊断”**模型，将一个恶性肿瘤（正类）误判为良性（负类）的代价极高。在这种场景下，你倾向于选择一个更大的 `C` 还是更小的 `C`？为什么？
    -   如果这是一个**“产品推荐”**模型，偶尔推荐错一个用户可能不感兴趣的产品，代价相对较低。这时，你的选择又会是什么？

通过这个简单的参数 `C`，SVM赋予了我们作为“架构师”一个强大的调控旋钮，让我们能够根据具体的业务需求，在模型的复杂度和泛化能力之间找到最佳的平衡点。

**总结与对比**

| 特性 | 逻辑回归 | 支持向量机 |
| :--- | :--- | :--- |
| **核心思想** | 概率拟合 (拟合S形曲线) | 几何分割 (最大化间隔) |
| **输出** | 属于某个类别的概率 (0-1) | 直接的类别判断 (属于哪一边) |
| **关注点** | 关注所有数据点 | 只关注最靠近边界的支持向量 |
| **可解释性** | 较好，输出的概率有业务意义 | 较差，决策边界的权重不易直观解释 |

这两种思路代表了分类问题中两种不同的哲学。逻辑回归更“圆滑”，它给出了一个带有不确定性的概率判断；而SVM则更“极致”，它追求的是最稳健、最安全的几何划分。
