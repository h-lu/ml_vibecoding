---
title: "6.5 Vibe Coding 实践：处理不平衡的世界"
---

在真实的商业世界里，我们关心的数据往往是“不平衡”的。预测设备故障、识别欺诈交易、发现潜在的癌细胞……在这些场景中，我们真正关心的“正例”（故障、欺诈、患病）通常只占数据总量的极小一部分。我们之前讨论的客户流失问题也是如此，大部分客户是留存的，只有少数会流失。

这种**类别不平衡 (Class Imbalance)** 的问题，是分类任务中最常见的陷阱之一。如果处理不当，我们可能会训练出一个表面上看起来“准确率”很高，但实际上毫无用处的模型。

在本节的Vibe Coding实践中，我们将直面这个挑战，学习如何像一个经验丰富的架构师一样，引导AI处理不平衡数据。

**任务描述**:
我们将使用一个电信客户流失数据集 `telecom_churn_imbalanced.csv`。在这个数据集中，“流失”客户是少数类。我们的目标是建立一个能够有效识别这些少数派的模型。

### 第一阶段：AI 暴露问题 (10分钟)

让我们从一个最直接、最天真的Prompt开始。我们将假装自己对类别不平衡一无所知，看看AI会怎么做。

::: {.callout-tip}
### Vibe Coding Prompt 1

“你好，请使用pandas加载名为 `telecom_churn_imbalanced.csv` 的数据集。然后，使用逻辑回归模型，基于所有特征来预测 `Churn` 标签。在建模前，请对数值特征进行标准化，对类别特征进行独热编码。最后，请在测试集上评估模型的**准确率 (Accuracy)** 并打印出来。”
:::

**观察与反思**:
将以上提示词交给 AI 助手。你将看到，AI可能会报告一个非常高的准确率，例如 **90%** 左右。这看起来是个很棒的结果！但它真的有用吗？

如果检查一下数据集中类别的比例，你会发现“不流失”(标签0)的客户占了大约90%。这意味着，如果一个模型什么都不学，只是无脑地将所有客户都预测为“不流失”，它也能达到90%的准确率！

这个看似“精准”的模型，对于我们识别流失客户的商业目标来说，完全是**无效**的。这就是不平衡数据下的“准确率陷阱”。

### 第二阶段：人类引导优化 (30分钟)

现在，轮到你——“系统架构师”——登场了。AI 给出的标准答案并不一定是商业上的最佳答案。你需要戴上“业务的眼镜”来审视这个模型，并提出更深刻的优化方向。

请思考以下几个引导性问题：

**1. 评估指标的“灵魂拷问”：哪种错误我们更无法容忍？**

“准确率”之所以有误导性，是因为它平等地看待了两种错误：

-   **错误A (False Positive)**: 把一个本不会流失的忠实客户，错误地预测为“会流失”。
    -   *商业后果*: 我们可能会给他发送一封不必要的挽留邮件或优惠券，产生少量营销成本。
-   **错误B (False Negative)**: 把一个真的要流失的客户，错误地预测为“不会流失”。
    -   *商业后果*: 我们对他毫无防备，眼睁睁地看着他流失，损失了这位客户未来的全部价值。

**你的任务 (向AI提问或自己思考)**:

-   在客户流失这个场景下，哪种错误的商业成本显然更高？
-   我们应该选择一个什么样的评估指标，来专门衡量我们“**找出了多少个真正要流失的客户**”的能力？（提示：这个指标叫“召回率”/Recall）
-   我们又该用什么指标来衡量“**在我们预测会流失的人里，有多少是真的会流失**”的能力，以控制营销成本？（提示：这个指标叫“精确率”/Precision）

**2. 数据处理的“时空法则”：如何避免数据泄露？**

一个常见的想法是：“既然少数类样本少，我多‘造’一些不就行了？” 这就是**过采样 (Over-sampling)** 的基本思想，其中最著名的技术叫 **SMOTE** (Synthetic Minority Over-sampling Technique)。它通过在少数类样本之间进行插值来创造新的、相似的“合成”样本。

但这里隐藏着一个致命的陷阱，无数初学者曾在此犯错。

**你的任务 (Vibe Coding 对抗性测试)**:

-   **第一步（错误示范）**: 请你的AI助手，**先对整个数据集进行SMOTE过采样，然后再将处理过的数据集划分为训练集和测试集**。看看模型的性能报告，特别是召回率，结果是不是看起来“好得令人难以置信”？
-   **第二步（正确做法）**: 现在，请AI助手**先划分训练集和测试集，然后只对训练集进行SMOTE过采样**。再看看模型的性能。
-   **对比思考**：为什么第一步的结果是虚假的？它犯了什么根本性错误？（提示：这被称为**“数据泄露”** (Data Leakage)，即让模型在训练时“偷看到”了本应属于测试集的信息。）

**3. 解决方案的权衡：是改变“数据”，还是改变“模型”？**

SMOTE是在数据层面“做手脚”。但我们也可以换一个思路：数据不动，能不能让**模型算法本身**在学习时，就对少数类“另眼相看”？

**你的任务 (从第一性原理思考)**:

-   逻辑回归优化的目标是最小化损失函数。如果我们想让模型更重视对“流失客户”的预测，我们应该如何修改这个损失函数？
-   翻阅一下`scikit-learn`中`LogisticRegression`的文档，找找看有没有一个参数，能让我们为不同类别设置不同的“惩罚权重”？（提示：这个参数叫`class_weight`）

通过这三轮“灵魂拷问”，你已经从一个被动的方案接收者，变成了一个主动的、批判性的系统设计者。现在，你已经准备好给出最终的、专业的指令了。

::: {.callout-tip}
### Vibe Coding Prompt 2

“非常好。现在，请重构我们的代码，遵循以下最佳实践：

1.  **使用 `imblearn.pipeline`** 来创建一个新的处理流水线。这个流水线需要包含三个步骤：我们之前的预处理器、一个`SMOTE`采样器(`imblearn.over_sampling.SMOTE`)，以及一个逻辑回归分类器。
2.  在逻辑回归分类器中，设置 `class_weight='balanced'` 参数。
3.  使用新的流水线来训练模型。
4.  在测试集上进行预测，并打印出**分类报告 (classification_report)** 和 **混淆矩阵 (confusion_matrix)**，让我们能看到更全面的评估结果。”
:::

**架构师的洞察**:
当 AI 生成代码并运行后，观察新的评估结果，你会发现：

-   **准确率 (Accuracy)** 可能比之前降低了，但这没关系！因为它现在是一个更诚实的指标。
-   **召回率 (Recall)** 对于类别1（流失客户）有了显著的提升。这意味着我们的新模型成功地**识别出了更多**真正会流失的客户，这正是我们想要的！
-   **精确率 (Precision)** 对于类别1可能有所下降，这意味着在被我们预测为“流失”的客户中，有一部分实际上不会流失。
-   **混淆矩阵**直观地展示了这一点：我们以“将一些好客户误判为流失客户”（左下角的数）为代价，换来了“成功识别出更多真正的流失客户”（右下角的数）。

这种权衡在商业上往往是值得的。因为对一个潜在流失客户进行一次多余的营销，其成本远低于完全错过他、让他流失所带来的损失。

通过这个实践，我们不仅解决了一个技术问题，更重要的是，我们学会了如何**从业务价值出发，选择合适的工具和指标，做出明智的架构决策**。这正是Vibe Coding和系统架构师思维的精髓。
