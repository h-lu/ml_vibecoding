---
title: "7.2 决策树：直观的判断逻辑与局限"
---

决策树 (Decision Tree) 是整个树模型家族的基石。它的核心思想极其简单，非常符合人类的思考方式：**基于一系列“如果...否则...”的规则来进行决策**。

想象一下银行家如何决定是否批准一笔贷款。他可能会进行如下的判断：

- **如果** 申请人年收入大于50万？
    - **是**：**如果** 他没有过往的违约记录？
        - **是**：批准贷款。
        - **否**：拒绝贷款。
    - **否**：**如果** 他拥有的资产大于100万？
        - **是**：批准贷款。
        - **否**：拒绝贷款。

这个判断过程就可以用一棵倒立的树来表示，这就是决策树。

```{mermaid}
graph TD
    A[年收入 > 50万?] -- 是 --> B{有违约记录?};
    A -- 否 --> C{资产 > 100万?};
    B -- 是 --> D[拒绝];
    B -- 否 --> E[批准];
    C -- 是 --> F[批准];
    C -- 否 --> G[拒绝];
```

这棵树由两种元素构成：

- **决策节点 (Decision Node)**：代表一个问题或一次测试（例如“年收入 > 50万?”）。
- **叶子节点 (Leaf Node)**：代表一个最终的决策或分类结果（例如“批准”或“拒绝”）。

决策树学习的目标，就是**从数据中自动地学习出这样一棵树，让它的决策规则能够尽可能准确地对样本进行分类或回归**。

### 核心问题：如何找到“最佳”分裂点？

决策树的生长过程，就是一个不断将混杂的数据集进行划分，使其变得越来越“纯净”的过程。

想象一下，我们有一堆混合在一起的红色和蓝色的小球。我们想找到一种方法，通过一系列“切分”，能最好地将红色和蓝色分开。

- **初始状态**：所有小球（数据点）都混在根节点，非常“不纯”。
- **目标**：我们希望每一次切分（每一次提问），都能让切分后的两个子集中的小球颜色尽可能地单一。最终，在叶子节点，我们希望所有的小球都是同一种颜色，即达到“最纯”的状态。

那么，如何用数学语言来衡量一个节点的“纯度”呢？有两个最常用的指标：**信息熵 (Entropy)** 和 **基尼不纯度 (Gini Impurity)**。

#### 1. 信息熵 (Entropy)

信息熵源于信息论，它衡量的是一个系统的不确定性或混乱程度。一个系统越混乱、越不确定，其信息熵就越大。

对于一个数据集 D，假设它有 K 个类别，第 k 个类别所占的比例为 \(p_k\)，那么数据集 D 的信息熵定义为：
$$
\text{Ent}(D) = - \sum_{k=1}^{K} p_k \log_2(p_k)
$$

-   **当 $p_k = 0$ 或 $p_k = 1$ 时**：即节点中所有样本都属于同一类别，系统没有任何不确定性，此时 $p_k \log_2(p_k) = 0$，信息熵为0，代表**最纯**。
-   **当 $p_k = 1/K$ 时**：即所有类别的样本数量完全相等，系统最混乱，不确定性最大，此时信息熵达到最大值。

决策树在选择分裂特征时，会计算使用某个特征 A 进行分裂后，系统信息熵的减少量，这个减少量被称为**信息增益 (Information Gain)**。决策树会选择那个能带来**最大信息增益**的特征作为当前节点的最佳分裂点。

#### 2. 基尼不纯度 (Gini Impurity)

基尼不纯度是另一个衡量数据不纯度的指标。它的物理意义是：从一个数据集中随机抽取两个样本，其类别标记不一致的概率。基尼不纯度越小，代表数据集的纯度越高。

对于一个数据集 D，其基尼不纯度的计算公式为：
$$
\text{Gini}(D) = 1 - \sum_{k=1}^{K} p_k^2
$$

-   **当所有样本属于同一类别时**：某个 $p_k=1$，其余为0，此时 $\text{Gini}(D) = 1 - 1^2 = 0$，代表**最纯**。
-   **当样本均匀分布在各个类别时**：不纯度最高。

与信息增益类似，决策树（特别是 `scikit-learn` 中默认使用的 CART 算法）会选择那个能让分裂后的**基尼不纯度下降最多**的特征和阈值作为分裂点。

*注：基尼不纯度和信息熵在实际效果上差别不大，但基尼不纯度的计算速度通常更快一些，因为它不涉及对数运算。*

### 单个决策树的“阿喀琉斯之踵”：过拟合

决策树有一个致命的弱点：如果不对其生长加以限制，它会倾向于**完美地拟合训练数据中的每一个点**，从而导致**过拟合 (Overfitting)**。

想象一下，这棵树会不断地生长，直到每个叶子节点只包含一个样本，或者所有样本都属于同一类。这样的模型在训练集上能达到100%的准确率，但它学到的规则会过于具体、过于复杂，失去了泛化到新数据上的能力。它把训练数据中的噪声和偶然性也当作了普适的规律来学习。

下面的交互式图表生动地展示了这个问题。我们使用一个简单的数据集（月亮形状的两个分类），并允许你通过滑块来控制决策树的**最大深度 (`max_depth`)**。

<iframe src="../assets/ch07/decision_tree_overfitting_moons.html" width="100%" height="620" frameborder="0"></iframe>

**请与上图互动，并思考：**

1.  **`max_depth` = 2 或 3**：观察决策树如何用几个简单的、横平竖直的“矩形”来近似地分割两个月亮。这时的决策边界比较平滑，虽然不能完美分开所有训练点，但它捕捉到了数据的大致分布，泛化能力可能较好。
2.  **`max_depth` = 4 或 5**：决策边界变得越来越复杂、越来越“扭曲”，试图去迎合那些零散的、可能是噪声的数据点。
3.  **`max_depth` = 6 (或更高)**：决策边界变得极其复杂，形成了许多孤立的“小岛”。这棵树几乎完美地记住了每一个训练样本，但这显然不是一个好的模型。如果一个新的数据点落在这些“小岛”附近，预测结果会非常不可靠。

这个实验直观地告诉我们：
- 单个决策树的能力**非常强大**，它理论上可以拟合出任何形状的决策边界。
- 但这种强大的能力也是一把“双刃剑”，使其极易**过拟合**。

因此，在实践中，我们几乎从不单独使用一棵深度不受限制的决策树。控制过拟合是使用树模型的核心挑战，常用的方法包括：
- **剪枝 (Pruning)**：限制树的生长，如限制最大深度 (`max_depth`)、限制叶子节点的最小样本数 (`min_samples_leaf`)、或者要求分裂必须带来足够大的收益 (`min_impurity_decrease` 或 `ccp_alpha`)。

然而，仅仅“砍掉”一棵树的分支是不够的。为了从根本上解决这个问题，并进一步提升模型的性能，机器学习大师们提出了一个更强大的思想：**集成学习 (Ensemble Learning)**。

在接下来的两节中，我们将看到如何通过“众人的智慧”——随机森林和梯度提升——来克服单个决策树的弱点，打造出稳定而强大的预测模型。
