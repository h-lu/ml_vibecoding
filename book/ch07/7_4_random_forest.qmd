---
title: "7.4 随机森林：Bagging 的力量"
---

**随机森林 (Random Forest, RF)** 是 Bagging 思想最杰出的代表。它完美地诠释了“三个臭皮匠，赛过诸葛亮”的集成哲学，并且通过引入一个巧妙的“随机性”来源，将 Bagging 的威力发挥到了极致。

如上一节所述，Bagging 的核心在于通过在略有不同的数据子集上训练多个独立的模型来降低方差。随机森林完全遵循这一流程，但它在“独立训练”这一步上，增加了一个关键的改动。

### 随机森林的“两个随机”

随机森林的强大之处，来源于它的**两个随机性来源**：

1.  **行抽样 (样本随机)**：这是 Bagging 自带的随机性。通过对训练样本进行有放回的自助采样 (Bootstrap)，创建出多个不同的训练集。这确保了每棵树学习的数据略有不同。

2.  **列抽样 (特征随机)**：这是随机森林**独有的、画龙点睛的一笔**。在训练每一棵决策树时，当需要在某个节点上寻找最佳分裂点时，随机森林**并不会在所有的特征中进行搜索**。相反，它会**随机地选择一小部分特征**（例如，总特征数的平方根 `sqrt(n_features)`），然后只在**这个随机的特征子集**中寻找最佳分裂点。

**为什么“特征随机”如此重要？**

想象一下，如果数据中存在一个**非常强**的预测特征（例如，在房价预测中，“房屋面积”可能就是这样一个强特征）。如果不进行特征随机，那么我们用 Bagging 训练的每一棵树，都很有可能在最顶层的节点就选择这个强特征进行分裂。

这会导致一个问题：所有树的结构都会变得非常相似，它们都由这个强特征主导。这样的“专家委员会”虽然成员众多，但观点高度一致，失去了多样性。当这个强特征的预测出现偏差时，整个模型都会跟着跑偏，Bagging “降低方差”的效果就会大打折扣。

而引入“特征随机”后，情况就不同了。在某些树的生长过程中，那个最强的特征可能根本没有被选入候选的特征子集。这“逼迫”这棵树去发掘和利用其他次要特征中的信息。因此，我们最终得到的森林中，每棵树都长得“各具特色”，有的擅长利用特征A和B，有的擅长利用特征C和D。**这种多样性，使得整个森林在做最终决策时更加稳健、方差更低。**

### 袋外误差 (Out-of-Bag Error, OOB Error)

随机森林的“行抽样”过程带来了一个非常实用的“副产品”：**袋外误差**。

由于我们进行的是有放回抽样，对于每一棵树来说，大约有 **36.8%** 的原始训练数据从未被它见过（这个概率可以通过数学极限 $\lim_{n \to \infty} (1 - 1/n)^n = 1/e \approx 0.368$ 推导得出）。这些未被用于训练的数据，被称为**袋外样本 (Out-of-Bag Samples)**。

我们可以利用这些袋外样本来得到一个无偏的、类似于交叉验证的模型性能评估。具体做法是：

1.  对于每一个原始训练样本，找到所有**没有用它进行训练**的树（即它作为袋外样本的那些树）。
2.  让这些树对这个样本进行一次“袋外预测”。
3.  将所有树的袋外预测结果进行聚合（投票或平均），得到该样本的最终袋外预测。
4.  计算所有样本的袋外预测与真实标签之间的总误差（例如，准确率或均方误差），这个误差就是**袋外误差 (OOB Error)**。

OOB Error 是对模型泛化能力的一个非常好的估计，它使得我们在训练随机森林时，**无需再额外划分出一个验证集**来进行模型选择或参数调优，从而可以利用所有数据进行训练，这在数据量较少时尤其有用。在 `scikit-learn` 的 `RandomForestClassifier` 中，只需设置 `oob_score=True` 即可自动计算。

### 决策边界的可视化对比

让我们再次回到上一节的“月亮”数据集，直观地看看“随机”的力量。下图对比了三种模型在该数据集上的决策边界：

<iframe src="../assets/ch07/rf_vs_tree_boundary_simple.html" width="100%" height="550" frameborder="0"></iframe>

**请对照上图阅读，并思考：**

1. **三幅子图（从左到右）：**

   * **Single Decision Tree (`max_depth=10`)**：黄色**实线**为 $p=0.5$ 的**决策边界**；浅蓝/浅红分别表示模型预测为**类0（留存）/类1（流失）**的区域。可以看到边界**曲折不规则**、到处是迎合噪声形成的“小岛”，典型**过拟合**。
   * **Ensemble of 15 Trees（No Randomness）**：这里做了 15 棵树的“集成”，但**没有行抽样也没有列抽样**（每棵树都在**同样的数据与特征**上训练，且基学习器是确定性的）。因此 15 棵树几乎**一模一样**，集成后的**决策区域与左图完全相同**——这直接说明了\*\*“多样性”才是 Bagging 有效的关键\*\*。
   * **Random Forest（100 Trees，Bootstrap + `sqrt` 特征）**：真正的随机森林，同时进行**行抽样（Bootstrap）**与**列抽样（随机特征）**。你会看到边界**更平滑、更加规整**，噪声引发的“小岛”明显减少。标题处给出了**OOB 准确率**，可作为无需留出验证集时的泛化性能近似。


这个对比清楚地表明：**随机森林**通过“**两个随机**”（样本与特征）制造**多样化**的基学习器，再用集成投票**中和单树的过拟合**，从而获得**更低方差、更稳健**的决策边界。


在下一节中，我们将转向集成学习的另一个极端——Boosting，看看它是如何通过一种完全不同的哲学来追求模型的极致性能的。
