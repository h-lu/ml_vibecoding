---
title: "5.2 线性回归：优雅的基石"
---

在确立了回归问题的本质是“寻找最佳函数”之后，让我们从最简单、最经典的假设开始：假设这个最佳函数是一个**线性函数**。这意味着我们假设因变量（如房价）和自变量（如面积、位置）之间存在着线性关系。这就是**线性回归 (Linear Regression)** 的核心思想。

### 几何直觉：最小化所有点到“线”的距离之和

让我们暂时只考虑一个特征：房屋的面积。我们的任务是在“面积-价格”的二维平面上，画一条直线，使其能最好地“代表”所有的数据点。

但什么是“最好”呢？

直觉上，这条线应该从数据点的“中间”穿过。线性回归为这个直觉提供了一个精确的数学定义，这个定义被称为**最小二乘法 (Ordinary Least Squares, OLS)**。

**最小二乘法的思想是**：最佳的回归线，是那条能让所有数据点到这条线的**纵向距离（也称为残差，Residual）的平方和**最小的线。

这个“距离”的定义非常关键。我们来看下面的互动动画。

<iframe src="../assets/ch05/linear_reg_demo.html" width="100%" height="520" frameborder="0"></iframe>

*请尝试拖动下方的“斜率”滑块，改变直线的位置。观察红色的残差线是如何变化的，并注意标题栏中的“残差平方和”。你的目标是，通过调整滑块，让这个值变得尽可能小。*

你会发现，无论你怎么调整，都很难超越右上方提示框中的“最佳解 (OLS)”。这个最佳解，就是 `scikit-learn` 这样的库通过高效的数学计算（而非手动尝试）为我们找到的全局最优解。当你觉得“玩”够了，可以点击“重置为最佳拟合线”按钮，直接查看最优解。

**为什么要用“平方”和？**
1.  **消除正负号**：残差有正有负（点在线的上方或下方），直接相加会相互抵消。平方确保了所有误差都是正数。
2.  **惩罚大误差**：一个大的误差（例如残差为4）在平方后会变成16，而两个小的误差（例如残差为2）平方和仅为 4+4=8。这意味着最小二乘法对“离群点”非常敏感，它会尽力调整直线来避免产生巨大的残差。
3.  **数学便利性**：平方和函数是一个光滑的凸函数，它有唯一的最小值，并且可以用微积分轻松地求出这个最小值的解析解。这使得计算变得非常高效。

### 物理类比：最小化系统的“总能量”

如果你觉得几何距离的解释还是有些抽象，可以尝试用一个物理系统来类比。

-   想象每一个蓝色的**数据点**都是一颗固定在墙上的**钉子**。
-   想象我们要找的那条**回归线**是一根很有弹性的**橡皮筋**。
-   残差（数据点到线的垂直距离）可以看作是连接钉子和橡皮筋的**小弹簧**。

现在，你把这根橡皮筋穿过所有的小弹簧，然后松手。橡皮筋会在弹簧的拉扯下上下振动，最终会停在一个**平衡位置**。这个平衡位置，就是使整个系统**总势能（所有弹簧的拉伸程度之和）最小**的位置。

这个位置，就精确地对应着我们用最小二乘法找到的那条最佳回归线。

### 延伸到多维空间

当我们的特征不止一个时（例如，同时考虑面积和位置），我们寻找的就不再是一条“线”，而是一个“平面”（两个特征）或“超平面”（两个以上特征）。但线性回归的根本思想完全没有改变：**我们依然是在寻找一个（超）平面，使得所有数据点到这个（超）平面的纵向距离（残差）的平方和最小。**

这个简单、优雅且强大的思想，构成了无数更复杂模型的基础。理解了它，就等于掌握了解锁回归世界的钥匙。
