---
title: "11.3 Transformer 宏观架构：组装一台“注意力机器”"
---

理解了自注意力（Self-Attention）这个核心部件，我们就可以开始组装一台完整的 Transformer 了。一个标准的 Transformer 模型，通常由一个**编码器 (Encoder)** 和一个**解码器 (Decoder)** 组成，而这两者都是由一层层堆叠起来的模块构成的。

我们首先聚焦于构成编码器的核心模块，它主要由以下几个关键组件巧妙地搭建而成。

![Transformer Encoder Block](https://jalammar.github.io/images/t/encoder_layer.png)
*图片来源: "The Illustrated Transformer" by Jay Alammar*

### 1. 多头注意力 (Multi-Head Attention)

如果我们只用一套 `Q, K, V` 权重矩阵去做自注意力，就好比只用一个“视角”去审视句子。但一个句子中的依赖关系是多层次的，例如，有些关系是关于语法结构的，有些是关于语义关联的。

**多头注意力 (Multi-Head Attention)** 机制，正是为了解决这个问题。它不再只学习一套 `WQ, WK, WV` 权重矩阵，而是并列地、独立地学习**多套**（例如，8套或12套）权重矩阵。

-   输入的词嵌入向量会被分别送入这8套独立的“注意力头 (Attention Head)”。
-   每一个头都会独立地执行一次完整的自注意力计算（QKV三步曲），并产生一个输出向量。这就像8个独立的“专家”，从8个不同的“子空间”或“角度”去分析和重组序列信息。
-   最后，这8个头输出的向量会被拼接（Concatenate）在一起，并通过一个额外的线性层进行整合，形成最终的输出。

这种机制极大地增强了模型的表达能力，使其能够同时捕捉到序列中不同层面、不同类型的依赖关系。

### 2. 位置编码 (Positional Encoding)

自注意力机制有一个非常重要的特性（或者说缺陷）：它本身是**无视顺序的**。在计算注意力权重时，一个词与所有其他词的交互是同时发生的，模型并不知道哪个词在前，哪个词在后。如果直接将词嵌入送入自注意力层，它得到的结果和一个被打乱顺序的“词袋”是完全一样的。

这对于语言这种高度依赖顺序的序列来说是致命的。为了解决这个问题，Transformer 的发明者提出了一种简单而聪明的解决方案：**位置编码 (Positional Encoding)**。

-   **核心思想**：在将词嵌入向量输入给编码器之前，先给每个词的向量，**加上**一个代表其在序列中位置的**位置向量**。
-   **实现方式**：这个位置向量不是学习来的，而是使用不同频率的正弦（sin）和余弦（cos）函数直接计算生成的。
    $$
    \begin{align}
    &PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{\text{model}}}) \\
    &PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}})
    \end{align}
    $$
    其中 `pos` 是词的位置，`i` 是向量的维度。这种设计的好处是，每个位置都有一个独一无二的编码，并且模型可以很容易地学习到不同位置之间的相对关系。

通过这种方式，位置信息就被巧妙地“注入”到了输入向量中，使得后续的自注意力层在计算时，能够间接地利用到词语的顺序信息。

### 3. 前馈网络 (Feed-Forward Network)

在每个多头注意力层之后，都会跟一个相对简单但非常关键的组件：一个**位置无关的前馈网络 (Position-wise Feed-Forward Network)**。

-   它就是一个我们在第九章深入学习过的**标准全连接神经网络 (FCN)**，通常包含两层线性变换和一次 ReLU 激活函数。你可以将 FCN 的作用，理解为一个**加工和提炼**的步骤：在注意力层通过“聚合”从整个序列中收集了相关信息之后，FCN 负责对这些高度情景化的信息进行一次强大的非线性变换，从而提取出更高级、更抽象的特征，为下一层或最终的输出任务做准备。

### 4. 残差连接与层归一化 (Residuals & Layer Normalization)

最后，为了能够成功地训练一个由很多层模块堆叠起来的深度网络，Transformer 还使用了两个在深度学习领域中非常关键的“技巧”：

-   **残差连接 (Residual Connection)**：这正是我们在第九章学习过的、用于解决深度网络退化问题的关键“神级装备”。在每个子层（如多头注意力层、前馈网络层）的输入和输出之间，都建立一个“直连通道”（Short-cut）。即，子层的最终输出是 `Sublayer(x) + x`。这极大地缓解了梯度消失问题，使得信息和梯度能够更顺畅地在网络中流动，是构建深度 Transformer 的基石。
-   **层归一化 (Layer Normalization)**：在每个残差连接之后，都会进行一次层归一化。它会对每个样本、每个层的输出向量进行归一化（使其均值为0，方差为1），这能有效地稳定训练过程，加速模型收敛。

---

::: {.callout-note title="架构师视角：工业界的效率优化"}
我们刚才学习的是 Transformer 的经典原始设计。在工业界，为了追求更高的效率和性能，研究者们也提出了一系列优化。

- **Multi-Query Attention (MQA)**：在标准的“多头注意力”中，每个头都有自己独立的 K 和 V 投影权重。MQA 是一种优化，它让所有的头共享同一套 K 和 V 权重，只保留各自独立的 Q 权重。这极大地减少了模型在推理时所需缓存（KV Cache）的内存占用，对于长序列生成任务尤其高效。
- **相对位置编码 (RoPE & ALiBi)**：我们学习的经典位置编码是“绝对”位置编码。但业界也发展出了更先进的“相对”位置编码方案，如 RoPE 和 ALiBi。它们不直接告诉模型“你在第5个位置”，而是通过修改注意力计算方式，让模型感知到“你和另一个词相距3个位置”，这种相对关系对于模型处理超长序列的泛化能力更有帮助。
:::

### 整体架构图

将以上所有组件组合起来，我们就得到了一个完整的 Transformer 编码器模块。一个完整的编码器，就是将这个模块**重复堆叠 N 次**（例如，BERT-base 堆叠了12次）。

```{mermaid}
graph TD
    subgraph "Encoder Module (1 of N)"
        A[Input Embeddings + Positional Encoding] --> B[Multi-Head Attention];
        B --> C{Add & Norm};
        A --> C;

        C --> D[Feed-Forward Network];
        D --> E{Add & Norm};
        C --> E;
    end

    E --> F[Output to next Encoder Module or Decoder];

    style B fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```
通过这样的设计，Transformer 成功地构建了一台强大、高效、可深度堆叠的“注意力机器”，为处理序列数据带来了革命性的突破。
