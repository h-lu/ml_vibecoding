---
title: "11.4 Vibe Coding 实践：可视化注意力"
---

理论学得再多，不如亲眼看一次。在本节的 Vibe Coding 实践中，我们将化身为一名“模型神经外科医生”，通过编程“打开”一个预训练好的 Transformer 模型的大脑，亲眼看一看它在处理句子时，那不可见的“注意力”究竟是如何流动的。

**我们的目标：**
- **掌握从预训练模型中提取注意力权重的方法**：学会使用 Hugging Face `transformers` 库，获取模型在进行推理时的内部状态。
- **学会解读注意力热力图**：将抽象的注意力权重矩阵，转化为直观的热力图，并从中分析出有意义的语言模式。
- **建立对模型行为的直观感知**：通过可视化，将前面所学的 QKV、多头注意力等抽象概念，与模型处理具体任务时的实际行为联系起来。


### 第一阶段：AI 快速生成可视化代码

我们的任务是，加载一个预训练好的 BERT 模型，输入一个精心设计的句子，然后提取出模型内部某一个注意力头的权重矩阵，并将其绘制成一张热力图。这些技术细节，非常适合交给 AI 助手来完成。

> **提示 (Prompt):**
>
> “你好，我需要用 Python 和 Hugging Face `transformers` 库来可视化 BERT 模型的自注意力权重。请帮我编写一个完整的脚本，完成以下任务：
>
> 1.  **加载模型**：加载预训练的 `"bert-base-uncased"` 模型和对应的分词器。在加载模型时，请务必在 `from_pretrained` 方法中设置参数 `output_attentions=True`，以便让模型返回注意力权重。
> 2.  **准备输入**：定义一个需要分析的句子，例如：`"The robot delivered the mail after it was fixed."`。使用分词器对这个句子进行编码，并转换为 PyTorch 张量。
> 3.  **获取注意力**：将编码后的输入传给模型，获取模型的输出。从输出中提取出注意力权重 `attentions`。这是一个包含了所有层、所有头的注意力矩阵的元组。
> 4.  **选择并处理权重**：
>     -   我们只分析**第一层 (layer 0)、第一个注意力头 (head 0)** 的权重。请从 `attentions` 元组中取出这个特定的权重矩阵。
>     -   这个矩阵的维度通常是 `(batch_size, num_heads, sequence_length, sequence_length)`。由于我们的批次大小是1，请用 `squeeze()` 方法移除多余的维度，得到一个 `(sequence_length, sequence_length)` 的二维矩阵。
> 5.  **可视化**：
>     -   使用 `matplotlib.pyplot` 和 `seaborn` 库来绘制这个二维矩阵的**热力图 (heatmap)**。
>     -   获取分词器转换后的词元列表（tokens），并将其作为热力图的 x 轴和 y 轴标签。
>     -   为图表添加合适的标题和标签。
>
> 请确保代码是完整且可以直接运行的。”


### 第二阶段：人类解读注意力图

AI 助手会为你生成一张精美的热力图。现在，轮到你这位架构师来解读这张图背后的秘密了。

这张热力图的**行**和**列**都代表了输入句子被分词后的各个词元 (Token)。矩阵中第 `i` 行、第 `j` 列的方格颜色越深，代表模型在更新第 `i` 个词元的表示时，对第 `j` 个词元的**注意力强度越大**。

![Attention Heatmap Example](https://i.stack.imgur.com/3oH4g.png)
*（上图为注意力热力图的示例，你的 AI 生成的图会类似这样）*

**请仔细观察你的热力图，并尝试回答以下引导性问题：**

1.  **代词消歧 (Pronoun Disambiguation)**
    -   在热力图的 y 轴上，找到代表代词 **"it"** 的那一行。
    -   沿着这一行观察，看看哪个（或哪些）列的方格颜色最深？
    -   模型是把最高的注意力权重分配给了 "robot"，还是 "mail"？
    -   这个结果是否符合你的语言直觉？这如何证明了注意力机制能够捕捉到长距离的语义依赖关系？

2.  **语法关系 (Syntactic Relations)**
    -   观察动词，例如 "delivered" 和 "fixed"。它们倾向于关注哪些词？是否能看到它们对各自的主语和宾语（例如，"robot" -> "delivered" -> "mail"）分配了较高的注意力？
    -   观察介词，例如 "after"。它关注的重点是什么？

3.  **特殊标记的作用 (Special Tokens)**
    -   BERT 的输入通常会在开头加上 `[CLS]` 标记，在结尾加上 `[SEP]` 标记。
    -   观察 `[CLS]` 所在的那一行。它是否对句子中的所有词都给予了比较平均的关注？这通常被认为是 `[CLS]` 标记在聚合整个句子的信息，以用于分类等下游任务。
    -   观察 `[SEP]`（分隔符）所在行和列的注意力模式。它通常扮演了什么样的角色？

**动手探索**：
尝试更换不同的句子，特别是那些包含复杂从句或歧义的句子，重复上述过程。你甚至可以尝试提取并可视化**不同层、不同头**的注意力矩阵。你会发现，不同的头往往会学习到不同的、有趣的注意力模式。例如，有些头可能专注于邻近的词，而另一些头则擅长捕捉长距离的语法关系。

通过这次实践，你不再是一个只能看到模型最终输出的“黑箱使用者”。你已经掌握了“透视”模型内部工作机制的能力，这是成为一名优秀机器学习系统架构师的关键一步。
