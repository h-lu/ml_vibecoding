# 第5章：回归问题：线性与非线性的优雅与局限

> *从线性回归到高级回归模型的第一性拆解*

## 学习目标

- **掌握回归问题的本质**：从第一性原理理解回归是寻找变量间“趋势”和“关系”的数学表达。
- **精通线性回归**：不仅会用，更能从几何（最小化距离）、物理（最小化能量）的角度解释其工作原理。
- **理解过拟合与正则化**：通过信息论的视角，直觉地把握模型复杂度、过拟合以及正则化（Lasso, Ridge）作为“惩罚项”的意义。
- **即时应用 XAI**：学会在构建回归模型后，立即使用 XAI 工具（如 SHAP）来解释模型的预测，理解每个特征的贡献。
- **实践 Vibe Coding 的回归工作流**：高效完成从模型构建、评估到解释的完整流程，并专注于设计与业务目标一致的自定义损失函数。

## 章节结构

### 5.1 商业挑战：预测未来

- **开篇商业挑战**：一家房地产公司需要根据房屋的特征（面积、位置、房龄等）来预测其售价。这是一个典型的回归问题。
- **第一性原理**：回归的本质是在高维的特征空间中，找到一个函数（一条线、一个曲面），使其尽可能地“贴近”所有的已知数据点（房屋样本），从而可以用这个函数来预测未知数据点（新房屋）的价值。

### 5.2 线性回归：优雅的基石

- **几何直觉**：在线性回归中，我们寻找的是一条直线（一维特征）或一个超平面（多维特征），使得所有数据点到这条线/这个平面的“几何距离”之和最小。这个“距离”通常是**最小二乘法 (Least Squares)** 定义的，即预测值与真实值之差的平方。
- **物理类比**：可以把数据点想象成一些固定住的钉子，回归线想象成一根橡皮筋。橡皮筋最终会停在使其总“势能”（拉伸程度）最小的位置，这个位置就对应着最小二乘解。
- **互动动画**：一个二维散点图，学生可以手动拖动一条直线，观察残差（Residuals，即误差）平方和的变化，直观感受“最佳拟合线”是如何被找到的。

### 5.3 过拟合的诅咒与正则化的解药

- **非线性扩展**：当数据点不是线性分布时，我们需要更复杂的模型（如多项式回归）来拟合。但这会带来**过拟合 (Overfitting)** 的风险——模型对训练数据拟合得“过于完美”，以至于把噪声也学了进去，导致其在未知数据上表现很差。
- **信息论视角**：过拟合的模型是一个“信息量过大”的模型，它试图记住每一个细节。**正则化 (Regularization)** 就是给模型的“复杂度”增加一个惩罚项。
    - **Lasso (L1 正则化)**：倾向于将不重要的特征的系数直接惩罚到零，从而实现**特征选择**。
    - **Ridge (L2 正则化)**：倾向于让所有特征的系数都变小，但不会变为零，从而使模型更**平滑**、更**稳健**。
- **可视化**：展示随着模型复杂度（多项式阶数）的增加，拟合曲线如何从欠拟合变为完美拟合，再到剧烈波动的过拟合。

### 5.4 即时引入 XAI：模型为何如此预测？

- **为什么需要 XAI**：对于商业决策者来说，一个“黑箱”的预测结果是不可信的。我们需要知道模型是根据哪些特征做出判断的。
- **介绍 SHAP (SHapley Additive exPlanations)**：
    - **核心思想**：源自博弈论，SHAP 值可以公平地将模型的“预测贡献”分配给每一个特征。
    - **可视化解读**：
        - **SHAP 力图 (Force Plot)**：展示对于单个预测，每个特征是如何将预测结果从基准值“推高”或“拉低”的。
        - **SHAP 摘要图 (Summary Plot)**：展示在整个数据集上，哪些特征最重要，以及它们对预测结果的影响是正向还是负向。
- **实践**：对刚刚建立的房价预测模型，立即生成并解读 SHAP 图，找出影响房价的最关键因素。

### 5.5 Vibe Coding 实践：从标准模型到业务对齐

- **任务描述**：继续房价预测的案例。
- **第一阶段：AI 快速实现 (10分钟)**
    - **提示 (Prompt)**：“使用 scikit-learn，为房价数据集构建一个 Lasso 回归模型。使用交叉验证来选择最佳的正则化强度 (alpha)。然后，使用 SHAP 库来解释模型的预测。”
- **第二阶段：人类优化与业务对齐 (30分钟)**
    - **引导性问题**：
        1.  **损失函数**：标准的均方误差 (MSE) 对所有错误一视同仁。但在现实中，将高价房预测低了（损失潜在收益）和将低价房预测高了（导致无人问津），这两种错误的商业成本是否相同？
        2.  **自定义损失**：我们能否设计一个新的损失函数，比如对“高价低估”的错误施加更大的惩罚？
    - **学生动手**：
        - 思考并写下一个非对称损失函数的伪代码。
        - **挑战**：尝试指导 AI 如何在 scikit-learn 的框架下实现或近似这个自定义的损失目标。
        - **Red Teaming AI**：故意在数据中加入一些与房价无关的“噪声特征”（e.g., “房屋门口的树叶数量”），观察 Lasso 模型是否能成功地将这些特征的系数惩罚到零，测试其特征选择的稳健性。

## 练习与作业

1.  **模型比较**：在同一个数据集上，分别训练线性回归、Lasso 回归和 Ridge 回归。比较它们的系数有何不同，并解释原因。
2.  **XAI 报告**：为你构建的房价预测模型撰写一份简短的 XAI 报告，目标读者是房地产公司的销售经理。用通俗的语言解释模型最重要的发现。
3.  **失败模式讨论**：讨论在哪些情况下，即使是复杂的回归模型也可能做出完全错误的预测？（e.g., 市场环境发生剧变，出现前所未有的新特征等）。
