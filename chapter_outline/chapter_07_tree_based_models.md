# 第七章：基于树的集成模型：从决策树到梯度提升

---

## 教学目标

- **第一性原理**: 理解决策树“分而治之”的核心思想，以及信息熵/基尼不纯度作为“最佳切分点”依据的本质。
- **架构师视角**: 掌握从单个决策树到Bagging（随机森林）和Boosting（梯度提升）的演进路径，理解每种集成方法试图解决的核心问题（方差 vs. 偏差）。
- **Vibe Coding实践**: 能够熟练运用Vibe Coding，对一个给定的业务问题（回归或分类），选择并调优合适的树模型，并使用SHAP进行深度解释。

---

## 章节内容大纲

- **7.1 商业挑战：非线性和特征交互的复杂决策**
  - 引入一个更复杂的商业场景，例如预测客户的“生命周期价值 (LTV)”，其中线性模型难以捕捉用户行为的复杂模式（如年龄和收入的交互作用）。
  - 直观展示线性模型的局限性，引出对非线性决策能力的需求。

- **7.2 决策树：最符合人类直觉的“规则清单”**
  - **第一性原理**: 从“IF...THEN...”规则出发，构建决策树的基本形态。
  - **核心算法**: 解释如何通过“信息熵”或“基尼不纯度”来寻找最佳切分点，可视化决策过程。
  - **优缺点**: 优点（可解释性强、符合直觉），缺点（容易过拟合）。
  - **可视化**: 绘制一棵真实的决策树，展示其决策路径。

- **7.3 集成学习：三个臭皮匠，赛过诸葛亮**
  - **Bagging (套袋法) - 降低方差**:
    - **核心思想**: 通过Bootstrap抽样，构建多个“略有不同”的决策树，然后通过“投票”来减少单个树的偶然性（方差）。
    - **随机森林 (Random Forest)**: 在Bagging的基础上，引入“特征随机”，进一步增强模型的多样性，使其更加稳健。
  - **Boosting (提升法) - 减小偏差**:
    - **核心思想**: 串行构建模型，让新的模型专注于“纠正”前面模型的错误。
    - **梯度提升决策树 (GBDT)**: 解释模型如何通过拟合“残差”来逐步提升性能。
    - **XGBoost, LightGBM**: 简要介绍这些业界顶级的GBDT实现，强调其在性能和效率上的优势。

- **7.4 XAI 再升级：解密“黑箱”树模型**
  - **SHAP for Tree Models**:
    - 介绍 `shap.TreeExplainer`，它比 `KernelExplainer` 更高效、更精确。
    - **全局解释**: 使用摘要图（Summary Plot）找出最重要的特征。
    - **局部解释**: 使用力图（Force Plot）解释单个复杂案例的预测原因。
    - **依赖图 (Dependence Plot)**: 探索单个特征与模型输出之间的非线性关系，以及与其他特征的交互效应。

- **7.5 Vibe Coding 实践：挑战 Kaggle 竞赛经典问题**
  - **场景**: 选择一个经典的 Kaggle 竞赛数据集（例如房价预测的高级版或信用卡欺诈检测），这些问题通常需要强大的非线性模型。
  - **第一阶段 (AI 初稿)**: 使用 Prompt 让 AI 快速构建一个 LightGBM/XGBoost 基线模型。
  - **第二阶段 (人类优化)**:
    - **引导性思考1 (超参数调优)**: 引导学生思考 LightGBM/XGBoost 中最重要的超参数（如 `n_estimators`, `learning_rate`, `max_depth`）分别控制什么（模型复杂度、学习速度），以及如何系统地进行调优（如使用 `GridSearchCV` 或 `Optuna`）。
    - **引导性思考2 (特征工程)**: 结合 SHAP 的发现，思考是否可以创造新的交叉特征来帮助模型学习。
    - **最终指令**: 给出一个包含超参数搜索和交叉验证的、更专业的 Prompt。

- **7.6 章节练习**
  - 概念题：对比随机森林和梯度提升的核心思想异同。
  - 实践题：给定一个新数据集，要求学生独立完成从模型选择、训练、调优到解释的完整流程。
  - 思考题：在什么业务场景下，你可能会选择可解释性稍差但性能更强的GBDT，而不是一个简单的决策树？